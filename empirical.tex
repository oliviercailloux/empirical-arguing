\RequirePackage[l2tabu, orthodox]{nag}
\documentclass[version=3.21, pagesize, twoside=off, bibliography=totoc, DIV=calc, fontsize=12pt, a4paper, french, english]{scrartcl}
\input{preamble/packages}
\input{preamble/redac}
\input{preamble/math_basics}
\input{preamble/math_mine}


\usepackage{silence}
\WarningsOff[natbib]


%I find these settings useful in draft mode. Should be removed for final versions.
	%Which line breaks are chosen: accept worse lines, therefore reducing risk of overfull lines. Default = 200.
		\tolerance=2000
	%Accept overfull hbox up to...
		\hfuzz=2cm
	%Reduces verbosity about the bad line breaks.
		\hbadness 5000
	%Reduces verbosity about the underful vboxes.
		\vbadness=1300

\title{Study argument attacks empirically}
\author{Olivier Cailloux}
\affil{Université Paris-Dauphine, PSL Research University, CNRS, LAMSADE, 75016 PARIS, FRANCE\\
	\href{mailto:olivier.cailloux@dauphine.fr}{olivier.cailloux@dauphine.fr}
}
\author{Pierre Bisquert}
\affil{Affiliation}
\author{Nicolas Salliou}
\affil{Affiliation}
\author{Rallou Thomopoulos}
\affil{IATE, Univ Montpellier, INRAE, Institut Agro, MONTPELLIER, FRANCE\\
	\href{mailto:rallou.thomopoulos@inrae.fr}{rallou.thomopoulos@inrae.fr}
}
\hypersetup{
	pdfsubject={Argumentation},
	pdfkeywords={Position paper, Formal argumentation theory, Review, Logic-based argumentation},
}

\begin{document}
\maketitle

\section{Introduction}
\label{sec:intro}
Formal Argumentation Theory (FAT) \cite{Dung95} relies on “attacks” between arguments to compute most acceptable standpoints, in the form of the so-called “extensions”. These attacks are typically formalized as a binary relation considered as representing the directed contradiction or incompatibility status of pairs of arguments. In the vast majority of the articles in formal argumentation theory, the attacks are considered known, given to the analyst as a starting point of the study. 

In this position paper, we want to argue in favor of extending the scope of formal argumentation theory by relaxing this assumption. In particular, we argue that attacks should not necessarily be considered as representing an objective status of contradiction, as given for example by inspection of logical representation of arguments. We will give methodological and practical reasons to consider it of great importance to consider the attack relation as representing the subjective state of mind of an individual; and to study methods for obtaining information about this relation empirically.

Although some authors seem to acknowledge incidentally that attack relations sometimes can’t be considered known (cite Besnard \& Hunter), as we will see, few works in FAT avoid this assumption. One may think of at least two reasons for this apparent gap. First, to the best of our knowledge, the reasons for doubting this assumption have not been analyzed clearly. Second, there might be a lack of an idea of where to start if one relaxes this assumption, perhaps fearing that these questions would reveal hopelessly complex and that would be of interest only to the specific field of natural language processing. We provide here (partial) answers to these two points: we give reasons for doubting this assumpion; and we draw paths to study attack relations considered subjective that do not relate to natural language processing.

Preference based approaches still considers an attack relation, and the preference is supplementary data on top of the attack relation. (TODO: check Besnard \& Hunter - Elements of Argumentation, Chapter 6: Considering the Audience.) \commentRT{Transition sur preference-based approaches \`a expliquer car tombe comme un cheveu sur la soupe. 1) Subjective = related to a given audience, 2) Audiences introduced in (contextual) preference-based frameworks, 3) Cite different works (PAF, VAF ...)}

We want to distinguish between two attitudes towards the attack relations. 

The most usual attitude is what we call the \emph{a priori} attitude, reviewed in this section: the attack relations are considered known. They are deducible using a systematic procedure from the arguments. This covers several cases:
\begin{itemize}
\item the case where the arguments are considered given under a logic form and this is used to deduce the attack relations;
\item the case where the input itself is considered to be the attack relation;
\item and the case where the input is considered a set of arguments in natural language (NL), and the attack relations are deduced from these NL arguments. (One can think of text form, video, …)
\end{itemize}

Another attitude consists in considering the attack relations as empirical. Under that view, the attacks are revealed by something else than the arguments themselves and a priori knowledge, such as the reactions of individuals to the arguments, as we will propose.

Mention the study of \cite{ICCS2018} on comparisons of attacks? \commentRT{Faut-il ajouter les id\'ees suivantes : dans Yun et al. 2018 (ICCS) \`a partir d'un m\^eme ensemble d'arguments initial, 3 proc\'edures syst\'ematiques sont tour \`a tour appliqu\'ees pour d\'efinir les attaques ; puis les extensions sont calcul\'ees ; enfin les r\'esultats des 3 cas sont \'evalu\'es empiriquement par plusieurs groupes. L'objectif \'etant d'\'evaluer l'intuitivit\'e des 3 d\'efinitions.}

Besnard \& Hunter (Chapter 3, p. 38 to 39): “Note that we do not assume any metalevel information about formulae. In particular, we do not assume some preference ordering or ‘‘certainty ordering’’ over formulae. This is in contrast to numerous proposals for argumentation that do assume some form of ordering over formulae. Such orderings can be useful to resolve conflicts by, for example, selecting formulae from a more reliable source. However, this, in a sense, pushes the problem of dealing with conflicting information to one of finding and using orderings over formulae, and as such raises further questions such as the following: Where does the knowledge about reliability of the sources come from? How can it be assessed? How can it be validated?” \commentRT{Qu'est-ce qu'on conclut de cette citation ? Que Besnard \& Hunter ne supposent pas l'existence de pr\'ef\'erences ? Qu'ils n'envisagent pas l'attaque subjective ?}
In Chapter 4, they argue for an enlarged view. Selectivity is important. Impractical to represent every argument; some arguments are similar and can be summarized; readers do not want or can’t read many arguments and want a summary. They give multiple examples of situations where it is important to take into account the audience attitude towards the presented arguments.

\section{The usual perspective (in FAT?) about attacks}
Here we describe succinctly the main paradigms that have been proposed in argumentation to model arguments and attack relations, focusing on the tactics used to obtain asymetric attacks (arguments such that one attack another one but not conversely).

\subsection{Classical logic}
In this section we consider the proposition described in the famous book by \citet{besnard_elements_2000} about argumentation on the basis of classical logic. We focus on their concept of trees, as it permits to determine which propositions are warranted. We simplify some of their definitions, for example, by defining trees over sets of formulas instead of over arguments, and show in footnotes the original definitions so that the reader can observe that our definitions are equivalent to the original propositions.

Let $\Delta$ be a finite set of formulas in predicate logic with the usual deduction operator $⊢$. 
Given $\Phi \subseteq \Delta$, let ${\land}\Phi \subseteq \Delta$ denote the set of formulas that are conjunctions of all the formulas of $\Phi$ in whatever order (thus $\phi \in {\land}\Phi$ iff $\Phi$ is the set of all the conjuncts of $\phi$).
We use $⊥$ for “falsity”, as is classical, thus, for example, $\Phi \cup \Psi ⊢ ⊥$ denote that the formulas in $\Phi$ and $\Psi$ cannot all be satisfied. We say that a set of formulas $\Phi$ is consistent iff $\Phi ⊬ ⊥$.

An argument is a pair $(\Phi, \alpha)$ with $\Phi \subseteq \Delta$, called the support, and $\alpha$ a formula not necessarily from $\Delta$, called the claim, such that $\Phi$ is a consistent minimal subset of $\Delta$ satisfying $\Phi ⊢ \alpha$ (thus $\Phi ⊬ ⊥$, $\Phi ⊢ \alpha$ and $\forall \Phi' \subset \Phi: \Phi' ⊬ \alpha$).

Given two sets of formulas $\Phi, \Psi \subseteq \Delta$, say that $\Psi$ is a canonical undercut for $\Phi$ iff $\Psi$ is a coherent minimal subset of $\Delta$ satisfying $\Phi \cup \Psi ⊢ ⊥$ (thus $\Psi ⊬ ⊥, \Phi \cup \Psi ⊢ ⊥$ and $\forall \Psi' \subset \Psi: \Phi \cup \Psi' ⊬ ⊥$).
\footnote{The authors define an undercut for an argument $(Φ, α)$ as an argument $(Ψ , ¬(φ_1 ∧\ldots ∧ φ_n))$ where $\set{φ_1, \ldots, φ_n} ⊆ Φ$, and define that an argument $(Ψ , ¬(φ_1 ∧ \ldots ∧ φ_n))$ is a canonical undercut for $(Φ, α)$ iff it is an undercut for $(Φ, α)$ and $(φ_1, \ldots, φ_n)$ is the canonical enumeration of $Φ$.
As canonical undercuts depend only on the supports of the arguments, we choose to define this concept as a relation over the sets of formulas rather than on arguments.
%It follows that $\Psi \subseteq \Delta$ is a canonical undercut for $\Phi \subseteq \Delta$ in our sense iff, writing $¬(φ_1 ∧ \ldots ∧ φ_n)$ for the canonical enumeration of $Φ$, the argument $(Ψ , ¬(φ_1 ∧ \ldots ∧ φ_n))$ is a canonical undercut for $(Φ, α)$ in their sense, for any $\alpha$ entailed by $\Phi$.
}

\begin{example}
	\label{ex:abstract}
	Consider $\Delta = \set{x, y, y → ¬x, z, z → ¬y}$. Examples of arguments include $(\set{x}, x)$, $(\set{y, y → ¬x}, ¬x)$ and $(\set{z, z → ¬y}, ¬y)$. 
	The only canonical undercut for $\set{x}$ is the set $\set{y, y → ¬x}$. 
	The canonical undercuts for $\set{y}$ are the sets $\set{z, z → ¬y}$ and $\set{x, y → ¬x}$. 
	The only canonical undercut for $\set{y → ¬x}$ is the set $\set{x, y}$. 
	The canonical undercuts for $\set{y, y → ¬x}$ are $\set{z, z → ¬y}$ and $\set{x}$.
\end{example}

Given some set $N$, we represent a tree rooted at $n \in N$ using a pair $(n, s)$ where $s \subseteq N × N$ represents its successor relation (with $s$ defining a tree, thus, $s$ connected and defining at most one path from the root to any $n' \in N$). 
Canonical undercuts permit to associate to any consistent $\Phi \subseteq \Delta$ its \emph{undercut tree} $U = (\Phi, u_\Phi)$, where nodes are sets of formulas, with $u_\Phi$ defined as follows. 
Given a node $\Psi$ in the tree, let $u_\Phi^{-1}[\Psi] \subseteq \Delta$ denote the union of its ancestors, including itself (the brackets denote the reflexive and transitive closure); and define the successors of $\Psi$, $u_\Phi(\Psi)$, as the canonical undercutters $\Psi'$ for $\Psi$ that satisfy $\Psi' \nsubseteq u_\Phi^{-1}[\Psi]$.
We say that an undercut tree rooted at $\Phi$ argues for $\alpha$ whenever $(\Phi, \alpha)$ is an argument (thus $\Phi ⊢ \alpha$ and $\forall \Phi' \subset \Phi: \Phi' ⊬ \alpha$).
(Note that an undercut tree is finite, as $\Delta$ is finite.)
\footnote{The authors define an argument tree for $\alpha$ as a tree of arguments, rooted at an argument $a_1 = (\Phi_1, \alpha)$, such that the children of a node $a = (\Phi, \beta)$ consist of all canonical undercuts $(\Psi, \gamma)$ for $a$ such that $\Psi$ is not a subset of $\Phi \cup \Phi_1 \cup \Phi_2 \cup … \cup \Phi_n$, where $(Φ_1, \beta_1), …, (Φ_n, \beta_n)$ denote the ancestors of $(\Phi, \beta)$.
As the authors observe, it follows that an argument tree arguing for $\alpha$ has all of its arguments (except for the root) claiming the negation of the support of its parent. As a result, we choose to consider only the sets of formulas as nodes of our trees, corresponding to the supports of their argument trees.
}

\begin{figure}
	\caption{The undercut tree arguing for $x$}
	\label{fig:utx}
	\begin{tikzpicture}
		\tikzset{every node/.style={draw, ellipse, execute at begin node=$,execute at end node=$}}
		\path node (Phi1) {\Phi_1 = \set{x}};
		\path (Phi1) ++(0, -2cm) node (Phi2) {\Phi_2 = \set{y, y → ¬x}};
		\path (Phi2) ++(0, -2cm) node (Phi3) {\Phi_3 = \set{z, z → ¬y}};
		\path[draw, <-] (Phi1) -- (Phi2);
		\path[draw, <-] (Phi2) -- (Phi3);
	\end{tikzpicture}
\end{figure}
\begin{example}[Continuing \cref{ex:abstract}]
	\label{ex:abstUnder}
	\Cref{fig:utx} shows the sole undercut tree arguing for $x$ in our running example. There is only one such tree as the only argument that can be built for $x$ from $\Delta$ is $(\set{x}, x)$. Accordingly, it is the tree $(\set{x}, u_{\set{x}})$, with root $\Phi_1 = \set{x}$. The set of children $u_{\set{x}}(\set{x})$ of the root is the singleton constituted by the only canonical undercut of $\set{x}$, that is, $\Phi_2 = \set{y, y → ¬x}$. 
	To compute the children of this node, $u_{\set{x}}(\Phi_2)$, observe that out of the two canonical undercuts for $\Phi_2$, only one uses some new formulas (that is, formulas that are not in $\Phi_1 \cup \Phi_2$). Thus, the unique child of $\Phi_2$ is $\Phi_3 = \set{z, z → ¬y}$. 
	Finally, $u_{\set{x}}(\Phi_3) = \emptyset$ as $u_{\set{x}}^{-1}[\Phi_3] = \Phi_1 \cup \Phi_2 \cup \Phi_3 = \Delta$.
	
	The unique undercut tree arguing for $¬x$, displayed in \cref{fig:utnx}, is $(\Phi_2, u_{\Phi_2})$ with $u_{\Phi_2}(\Phi_2) = \set{\Phi_1, \Phi_3}$ and $u_{\Phi_2}(\Phi_1) = u_{\Phi_2}(\Phi_3) = \emptyset$.
\end{example}
\begin{figure}
	\caption{The undercut tree arguing for $¬x$}
	\label{fig:utnx}
	\begin{tikzpicture}
		\tikzset{every node/.style={draw, ellipse, execute at begin node=$,execute at end node=$}}
		\path node (Phi2) {\Phi_2 = \set{y, y → ¬x}};
		\path (Phi2) ++(-2cm, -2cm) node (Phi1) {\Phi_1 = \set{x}};
		\path (Phi2) ++(2cm, -2cm) node (Phi3) {\Phi_3 = \set{z, z → ¬y}};
		\path[draw, <-] (Phi2) -- (Phi1);
		\path[draw, <-] (Phi2) -- (Phi3);
	\end{tikzpicture}
\end{figure}

Given a tree $U = (\Psi, u_\Psi)$, define the set $R$ of defeated nodes as the set such that $r \in R$ iff $u_\Psi(r) \nsubseteq R$, thus, if some successor of $r$ in $U$ is not in $R$.
One can see that this defines a unique set by observing that the definition determines the status of each node starting from the leaves up to the root: the leaves are undefeated, the predecessors of some leaf are defeated, and so on.

\begin{example}[Continuing \cref{ex:abstUnder}]
	The root of the tree $(\set{x}, u_{\set{x}})$ arguing for $x$ is undefeated while the root of the tree $(\Phi_2, u_{\Phi_2})$ arguing for $¬x$ is defeated.
\end{example}

The authors propose to use the status of root nodes as one way of discriminating arguments.

\subsection{PAFs}
\label{sec:pafs}
Here we talk about Preference-based Argumentation Frameworks.

Considering the book Rahwan, Simari - Argumentation in Artificial Intelligence (2009), Chapter 15: Argumentation for Decision Making, Amgoud.

We assume that there is only one alternative on which to decide: $\dy$ or $\dn$ (e.g., “surgery” or “do nothing”). This simplifies the formal exposition without removing the crucial points of the propositions.

A decision system is $A_e, A_p^{\dy}, A_p^{\dn}, {≥_e}, {≥_p}, R_e, R_m$. 
\footnote{I assume that ${≥_m} = A_e × A_p$. Her section 2.2 suggests ${≥_m} = A_e × A_p$, Example 15.5 suggests ${≥_m} \subseteq A_e × A_p$; I assume the mistake is in the example. Also, $Def_m = R_m$ and $Def_e = R_e \cap (\overline{>_e})^{-1}$. $(\overline{>_e})^{-1}$ contains the pairs $(a, b)$ such that $a$ is strictly preferred to $b$ or they are indifferent or incomparable. $Def = (R_e \cap (\overline{>_e})^{-1}) \cup R_m = R.$}
Define $A_p = A_p^ {\dy} \cup A_p^{\dn}$.
Define $\allargs = A_e \cup A_p$ as the set of all arguments.

${≥_e} \subseteq A_e × A_e$, ${≥_p} \subseteq A_p × A_p$, both reflexive and transitive, the preferences.

$R_e \subseteq A_e × A_e$, $R_m \subseteq A_e × A_p$, the “objective” attacks. 

From $R = (R_e \setminus {<_e}) \cup R_m$, we obtain the extensions (an extension is a set of arguments). The stable and the preferred semantics are considered. Then the skeptically accepted arguments are $\cap E_i$ and the credulously accepted ones are $\cup E_i \setminus \cap E_i$.
\footnote{More from Amgoud: skept pref subseteq skept stable if not empty.}

$E$-acceptable = $E$-defended = all args that $E$ defends = those whose attackers are all attacked by $E$ = $\set{a \suchthat R^{-1}(a)\subseteq R(E)}$.

Conflict-free: contains no arguments that attack each other.
\footnote{More from Baroni and Giacomin: Admissible $E$: conflict-free and all included are $E$-defended; Complete $E$: $E$ admissible and includes every $E$-defended; Preferred: maximal admissible set, equivalently, maximal complete extensions. Stable: conflict-free and $E$ attacks all excluded args. Implies complete.}

Complete $E$: $E$ conflict-free and $E$ = $E$-defended.

Stable: conflict-free and $E$ attacks all excluded args. Implies maximally complete.

Preferred: maximal complete. Stable implies preferred. 

The semantics considered is: the stable ones if some exist; the preferred ones otherwise. Define $S$ as the skeptically accepted arguments in the resulting semantics, thus, $\cap E_i$ with the $E_i$ sets being the stable ones, or, if there is none, the preferred ones.

Given a set $A \subseteq \allargs$, say that $A$ is possibly failing iff $\exists B \subseteq \allargs$ with $B$ admissible and attacking every member of $A$ ($A \subseteq R(B)$).
Inversely, $A$ is necessarily resistant iff it is not possibly failing.

Interesting examples. $(5, 3), (3, 2), (2, 4), (4, 3), (2, 1), (1, d), (4, d)$. Here, 5 invalidates 3, which “suffices” to “resolve” the cycle as $(3, 2)$ “does not count” any more. Thus, 2 defends $d$ and $d$ is skeptically accepted. Also, consider: $(5, 4), (4, 3), (3, 2), (2, 4), (3, 1), (4, 1), (1, d)$. Without 5, $d$ is not sceptically accepted because 1 may be considered to resist thanks to the undecided status of the cycle, but with 5, $d$ becomes sceptically accepted as 1 is defeated.

\subsection{Defeasible logic}
\commentOCf{TODO: formalisme de base de la logique defeasible. Pierre nous fournit un article ou livre de base s’il en trouve un qui semble adéquat. Thèse suggérée : \url{https://hal.inria.fr/tel-01904558v2}}

Consider a propositional language $\mathcal{L}_p$ with the implication and conjunction connectives ($\rightarrow$,$\Rightarrow$,$\curly$,$\land$) and strong negation ($\lnot$) on a finite set of literals (facts) where a literal is either an atomic proposition or its negation. 
The set of facts is denoted $F$.

A fact in $\mathcal{L}_p$ is a literal; the body and head of a rule are finite conjunction of literals. A rule expressed in $\mathcal{L}_p$ is applicable on a set of literals if its body is included in this set.
There are three kinds of rules:
\begin{itemize}
\item \emph{strict rules}, denoted $\Phi \rightarrow \varphi$, indicating definite implications; we denote by $R_s$ the set of strict rules;
\item \emph{defeasible rules}, denoted $\Phi \Rightarrow \varphi$, indicating plausible implications; we denote by $R_d$ the set of defeasible rules;
\item \emph{defeater rules}, denoted $\Phi \curly \varphi$, preventing defeasible implications; we denote by $R_{df}$ the set of defeater rules;
\end{itemize}
The set of all rules is denoted $R$, the conclusion of a rule $r$ is denoted $Head(r)$ and its body $Body(r)$.

In order to express higher plausibility for some defeasible implications, a (generally acyclic) preference relation $\succ$ on $R$ is needed; $r_1 \succ r_2$ indicates that $r_1$ is superior to $r_2$ (and $r_2$ inferior to $r_1$).

A defeasible theory $T$ is a triple $(F,R,\succ)$ which will assign a label to any literal $f$ according to its provability:
\begin{itemize}
\item $+\Delta f$: $f$ is provable using only strict rules and facts in $T$,
\item $-\Delta f$: it is proved that $f$ is not strictly provable in $T$,
\item $+\delta f$: $f$ is provable using at least one defeasible rule or fact in $T$,
\item $-\delta f$: it is proved that $f$ is not deeasibly provable in $T$.
\end{itemize}

The general idea of defeasible logics is then to provide a proof for a given conclusion (fact). A proof is a finite sequence $P = (P(1), \dots, P(n))$ of labeled literals ($P(1..i)$ denotes the initial part of the proof P of length $i$) satisfying the following conditions \cite{Billington1993}:

\begin{itemize}
\item $+\Delta$: if $P(i+1) = +\Delta f$ then either
  \begin{enumerate}
  \item $f \in F$ or
  \item $\exists r \in R_s$ s.t. $f \in Head(r)$ and $\forall \varphi \in Body(r): +\Delta \varphi \in P(1..i)$.
  \end{enumerate}

\item $-\Delta$: if $P(i+1) = -\Delta f$ then
  \begin{enumerate}
  \item $f \notin F$ and
  \item $\forall r \in R_s$ s.t. $f \in Head(r)$, $\exists \varphi \in Body(r): -\Delta \varphi \in P(1..i)$.
  \end{enumerate}

\item $+\delta$: if $P(i+1) = +\delta f$ then either
  \begin{enumerate}
  \item $+\Delta f \in P(1..i)$ or
  \item \vspace{-0.1cm}
    \begin{enumerate}
    \item $\exists r \in R_s \cup R_d$ s.t. $f \in Head(r)$ and $\forall \varphi \in Body(r): +\delta \varphi \in P(1..i)$ and
    \item $-\Delta \lnot f \in P(1..i)$ and
    \item $\forall r' \in R$ s.t. $\lnot f \in Head(r')$, either:
      \begin{enumerate}
      \item $\exists \varphi \in Body(r')$ s.t. $-\delta \varphi \in P(1..i)$ or
      \item $\exists r^{\prime\prime} \in R$ s.t. $f \in Head(r^{\prime\prime})$ and $\forall \varphi \in Body(r^{\prime\prime}): +\delta f \in P(1..i)$ and $r^{\prime\prime} \succ r'$.
      \end{enumerate}
    \end{enumerate}
  \end{enumerate}

\item $-\delta$: if $P(i+1) = -\delta f$ then
  \begin{enumerate}
  \item $-\Delta f \in P(1..i)$ and
  \item \vspace{-0.1cm}
    \begin{enumerate}
    \item $\forall r \in R_s \cup R_d$ s.t. $f \in Head(r)$, $\exists \varphi \in Body(r): -\delta \varphi \in P(1..i)$ or
    \item $+\Delta \lnot f \in P(1..i)$ or
    \item $\exists r' \in R$ s.t. $\lnot f \in Head(r')$, and:
      \begin{enumerate}
      \item $\forall \varphi \in Body(r')$, $+\delta \varphi \in P(1..i)$ and
      \item $\forall r^{\prime\prime} \in R$ s.t. $f \in Head(r^{\prime\prime})$, either $\exists \varphi \in Body(r^{\prime\prime}): -\delta f \in P(1..i)$ or $r^{\prime\prime} \not\succ r'$
      \end{enumerate}
    \end{enumerate}
  \end{enumerate}
\end{itemize}

A fact $f$ is provable in $T$, denoted $T \models f$, if and only if there exists a proof $P$ in $T$ s.t. $f \in P$. For instance, intuitively, for $f$ to be strictly proven there needs to exist a strict rule $r \in R_s$ s.t. $f \in Head(r)$, and for any literal $l \in Body(r)$, there exists a strict rule $r' \in R_s$ s.t. $l \in Head(r')$, and so on recursively until a fact $t \in F$ is reached.

Ambiguity is the situation where a conflict between rules arises and the superiority relation does not indicate which rule should be used. Different intuitions on how ambiguity should be solved have been introduced over the years; these intuitions modify subtly the way $+\delta$ and $-\delta$ are computed in order to account for how ambiguity, e.g., should be propagated to other facts. The interested reader might refer to \cite{Hecham2018} for both an intuitive and formal overview.


\subsection{From text to logic arguments}
Should we devote a section to discuss proposals to “translate” NL to logic?

Arguments are supposedly formalized making the enthymemes explicit (“An enthymeme is a form of reasoning in which some premises are implicit, most often because they are obvious” \citep[p.\ 41]{besnard_elements_2000}). They give the example “The baby no longer has her parents; therefore, she is an orphan”, which is formalized by making the further implicit claim explicit: “if a baby no longer has her parents, then she is an orphan”, which makes it a correct reasoning. 

From Besnard \& Hunter - Elements of Argumentation, Chapter 1. “Numerous textbooks have explained how individual arguments, as found in the real world, can be represented and analyzed by classical logic. The focus of these books is on what constitutes a valid logical argument. A paragon of such a textbook by Fisher [Fis88] includes many examples of arguments originally presented in philosophy and in politics using free text, together with a comprehensive explanation of how they can be translated into propositional logic. However, these textbooks tend to circumvent the more difficult issues of inconsistency, conflict, and counterarguments.
To address these more difficult issues, an excellent starting point for considering monological argumentation is Toulmin’s book [Tou58], with developments of Toulmin’s approach having been reviewed by van Eemeren et al. [vGK87]. Toulmin’s work is regarded as a precursor to much of the formal developments in argumentation systems found in the artificial intelligence field. For reviews of formalisms for argumentation systems in artificial intelligence, see [PV02, CML00].” And more about this in their Section 4.9.

\section{Reasons not to consider it known}
We saw that most formal approaches in argumentation consider that the attack relation is (are) known. 
We here point out reasons to lift this assumption.

More precisely, we want to challenge the assumption, left implicit in the literature, that some procedure exists that permits to extract from texts in natural language non-trivial conclusions, and that makes no assumption on the quality or content of the arguments in the text.

\subsection{A case study}
To transmit intuition, we give here a detailed example of a situation where 
two interpretations of given texts are natural, and yet lead to different 
conclusions. The example is taken from a timely social and scientific topic, namely the use of insect breeding as an ecological way to recycle and valorize bio-waste, known as ``entomoconversion'' \citep{FAO2021,Kialo2019}.

\subsubsection{Case description}
This year on 3 May 2021, the European Union approved a first insect, Tenebrio molitor, as novel food. This comes after several years of emulation and initiatives around entomoconversion in Western coutries, while in other parts of the world insect consumption has been practiced for centuries.

Entomoconversion is not only considered for its potential in food provision, but also as an ecological way to sanitize and recycle biowaste, such as urban waste, for various uses from feed and food to chemicals, to name a few. On the other hand, as an innovation it raises questions concerning the legal framework regulating this activity, the technical issues related to its implementation, the safety of the products delivered, the conditions of its economic viability, etc. Networks, projects, debates and literature provide documentation about these various aspects.

Part of these concerns are captured by the 2 models presented below.

The difference between models 1 and 2 is that model 2 makes explicit the economic consequence of increased costs. This modifies the argument ECO2 of model 1 into ECO2’ in model 2, which generates additional attacks.

\subsubsection{Model 1}
Arguments:
\begin{description}
	\item[COM1] ( reducedProductAttractivity $→$ reducedSells )  $→$  reducedEconomicBenefit
	\item[ECO1] ( biowasteUse $→$ reducedCosts )  $→$  increasedEconomicBenefit
	\item[ECO2] ( scalingUp $→$ (buildingCosts $\land$ labourCosts $\land$ automationCosts) )  $→$  increasedCosts
	\item[ECO3] ( highInsectGrowthRate $→$ highYield )  $→$  increasedEconomicBenefit
	\item[ECO4] ( highInsectFertility $→$ highYield )  $→$  increasedEconomicBenefit
\end{description}

Logical contradictions:
\begin{itemize}
	\item reducedCosts, increasedCosts $→ ⊥$
	\item reducedEconomicBenefit, increasedEconomicBenefit $→ ⊥$
\end{itemize}

Attacks: see \cref{fig:m1}.
\begin{figure}
	\caption{Attacks resulting from model 1}
	\label{fig:m1}
	\begin{tikzpicture}
		\tikzset{every node/.style={draw, ellipse}}
		\path node (COM1) {COM1};
		\path (COM1) ++(-3cm, -2cm) node (ECO1) {ECO1};
		\path (COM1) ++(0, -2cm) node (ECO3) {ECO3};
		\path (COM1) ++(3cm, -2cm) node (ECO4) {ECO4};
		\path (COM1) ++(0, -4cm) node (ECO2) {ECO2};
		\path[draw, line width=2pt, <->] (COM1) -- (ECO1);
		\path[draw, line width=2pt, <->] (COM1) -- (ECO3);
		\path[draw, line width=2pt, <->] (COM1) -- (ECO4);
		\path[draw, line width=2pt, ->] (ECO2) -- (ECO1);
	\end{tikzpicture}
\end{figure}

\subsubsection{Model 2}
Arguments include COM1, ECO1, ECO3, ECO4, but ECO2 changes to:
\begin{description}
	\item[ECO2’] [ ( scalingUp $→$ (buildingCosts $\land$ labourCosts $\land$ automationCosts) ) $→$ increasedCosts ] $→$ reducedEconomicBenefit
\end{description}

Logical contradictions are the same.

Attacks: see \cref{fig:m2}.
\begin{figure}
	\caption{Attacks resulting from model 2}
	\label{fig:m2}
	\begin{tikzpicture}
		\tikzset{every node/.style={draw, ellipse}}
		\path node (COM1) {COM1};
		\path (COM1) ++(-3cm, -2cm) node (ECO1) {ECO1};
		\path (COM1) ++(0, -2cm) node (ECO3) {ECO3};
		\path (COM1) ++(3cm, -2cm) node (ECO4) {ECO4};
		\path (COM1) ++(0, -4cm) node (ECO2) {ECO2’};
		\path[draw, line width=2pt, <->] (COM1) -- (ECO1);
		\path[draw, line width=2pt, <->] (COM1) -- (ECO3);
		\path[draw, line width=2pt, <->] (COM1) -- (ECO4);
		\path[draw, line width=2pt, <->] (ECO2) -- (ECO1);
		\path[draw, line width=2pt, <->] (ECO2) -- (ECO3);
		\path[draw, line width=2pt, <->] (ECO2) -- (ECO4);
	\end{tikzpicture}
\end{figure}

\subsubsection{Consequence of model differences}
Presented to an expert consortium \citep{Insect4City2020} using the preferred semantics of acceptability, Model 1 highlights argument ECO2 as a major argument of the debate, unattacked, skeptically accepted. It clearly weakens ECO1 which is attacked by ECO2 and undefended. It tends to orient the debate on the need for precise evaluation of scaling-up costs --which is the focus of argument ECO2.

On the other hand, Model 2 puts the question of costs back into the more general context of economic benefit. In this view, argument ECO2' (the new version of ECO2 in Model 2) is now faced to several arguments addressing economic benefit concerns, namely ECO1, ECO3 and ECO4, in a symmetric confrontation. This totally changes its status in the debate, which is now balanced between pessimistic arguments regarding entomoconversion's economic benefit (COM1, ECO2') and optimistic ones (ECO1, ECO3, ECO4), with a numeric advantage for the latter.

The differences between both model conclusions may be extremely puzzling for end-users who are not argumentation specialists. In this case, pedagogy and explanation of modeling choices and hypotheses are needed in order not to discredit the model.

\subsection{Example “short contraposition”}
\commentOCf{Exemple trouvé par Pierre (orca.cf.ac.uk/84295/1/short\_contraposition.pdf) à décrire.}

\subsection{A contrast}
Our first point is that asymetric attacks do not stem from logic alone. It requires some opinion about the quality of the arguments, about which point of view is better defended, more grounded.

This can be illustrated by contrasting two examples with a similar structure. The first example is designed to make the conclusion feel intuitively justified, even though an (unreasonable) counter-argument is raised against it; whereas the second example features a similar situation from an argumentation attack point of view but where the conclusion feels intuitively unjustified.

\begin{example}[Ballistic expert]
	\label{ex:ball}
	Assume that a ballistic expert says that an impact on a wall in a given context can be best explained only by a bullet having been fired from some given position in the room. A counter-argument is raised saying that this in fact requires to accept the classical laws of physics (as known in the scientific community), but they in fact are false: a gigantic plot has led to them being falsely viewed as empirically validated, but they are not.
\end{example}
A naïve model of this disagreement can be designed as follows.
\begin{example}[Model for \cref{ex:ball}, with non-rebutted attack]
	\label{ex:ball1}
	This disagreement can be modeled using the atoms $\mathit{lp}$ for \emph{The classical laws of physics are correct} and $\mathit{b}$ for $\emph{A bullet has been fired from position … in the room}$.
	$\Delta = \set{\mathit{lp},\allowbreak \mathit{lp} → \mathit{b}, ¬\mathit{lp}}$.
	The undercut tree arguing for $\mathit{b}$ has root $\set{\mathit{lp},\allowbreak \mathit{lp} → \mathit{b}}$, having as child $\set{¬\mathit{lp}}$.
\end{example}
This model seems to not adequately capture the featured disagreement: the non rebutted attack from $¬\mathit{lp}$ against the conclusion of the expert suggests that its conclusion has been defeated and that the ballistic expert is wrong, as illustrated by the root of the undercut tree being unwarranted. On the contrary, it is very reasonable to assume that the very fact that the expert uses the classical laws of physics in her reasoning implicity indicates that she considers them correct. 
\begin{example}[Model for \cref{ex:ball}, with rebutted attack]
	\label{ex:ball2}
	We add the atom $\mathit{lp}'$ for \emph{The classical laws of physics are obviously correct}.
	$\Delta' = \set{\mathit{lp},\allowbreak \mathit{lp} → \mathit{b}, ¬\mathit{lp}, \mathit{lp}', \mathit{lp}' → \mathit{lp}}$.
	The undercut tree arguing for $\mathit{b}$ now has three nodes: root $\set{\mathit{lp},\allowbreak \mathit{lp} → \mathit{b}}$, having as child $\set{¬\mathit{lp}}$, having as child $\set{\mathit{lp}'}$.
\end{example}

Note that a model similar to \cref{ex:ball2}, with identical conclusions, is obtained by considering that the expert, instead of claiming implicitly $\mathit{lp}'$, claims that the claim of her opponent is false, thus, replacing $\set{\mathit{lp}', \mathit{lp}' → \mathit{lp}}$ by $¬¬\mathit{lp}$.

The coexistence of \cref{ex:ball1,ex:ball2}, two apparently plausible models but that suggest very different conclusions, raise a problem for the modeler: choosing one seems to require to abandon an a priori neutrality about “who is right”. 

One apparent way out of this conundrum is to “stick to the text”. By sticking to the text, we mean the strategy that consists in considering only arguments that explictly appear in the raw source. Under that approach, if the ballistic expert did not effectively answer that “The classical laws of physics are obviously correct” to the person giving the counter-argument, then, this is not included in the set of formulas. This strategy fails for two related reasons. First, it does not achieve the goal we are interested in here, of summarizing the arguments as best possible for displaying to a decision maker. When the goal is to display the arguments related to some issue, it is arbitrary to decide not to include an argument just because it has not been effectively uttered, if it can reasonably be thought that the argument would have been uttered, had the debater have had the possibility to say it. Second, it raises a difficult problem about where to stop. As \citeauthor{besnard_elements_2000} observe, it is often necessary to add implicit statements to a text in natural language for it to make logical sense. People may omit obvious implications; may use “if” when obviously meaning “if and only if”, may use “or” with the obvious meaning “exclusive or”, and so on. Sticking to the text without attempting at all to interpret what the protagonists really mean would yield pedantic and uninteresting debates. It seems like the modeler should have the right to deviate from the raw text at least to consider what the debater “obviously mean”. In this example, the expert can reasonably be interpreted as obviously meaning that her use of classical physics law is legitimate. Whether or not she explicitly said it during the exchange of arguments should not be relevant.

Another apparent solution is, in the name of neutrality, to systematically prefer the models which do not make any conclusion warranted, in case of possible hesitation. In our ballistic example, this would lead to favor \cref{ex:ball1}. One reasoning in favor of that choice is that when going to \cref{ex:ball2}, we might as well go one step further and define $\Delta^{\prime\prime} = \Delta' \cup \set{\mathit{¬lp'}}$, reflecting the arguably implicit position of the second protagonist that the claim “The classical laws of physics are obviously correct” is incorrect, and yielding again that $b$ is unwarranted. At this stage, this reasoning suggests that we might as well stick to \cref{ex:ball1}, which is simpler and seems to avoid redundant arguments. The problem with this approach is that the problem is general, as can be easily guessed intuitively (and made formal by the theorem below): any non-consensual situation, thus, situation where $\Delta$ permits to deduce both $\phi$ and $¬\phi$, suffers from the problem considered here, hence, following systematically the strategy of preferring models that leave propostions unwarranted in case some non-neutral choice is required would lead to trivial models only, which leave everything unwarranted except on those issues on which every protagonist agree from the start. 

We now turn to a similar example situation, where the opposite conclusion seems to be intuitively desired (that of making the claim unwarranted, instead of warranted), to further illustrate the difficulty of choice that the modeler faces.

The following example features the famous Tweety, where we wonder whether Tweety can fly.
\begin{example}[Full Tweety]
	\label{ex:fullTweety}
	Our language uses the atoms 
	$\mathit{fb}$ for \emph{Birds fly}; 
	$\mathit{bT}$ for \emph{Tweety is a bird}; 
	$\mathit{fT}$ for \emph{Tweety flies}; 
	$\mathit{bp}$ for \emph{Penguins are birds}; 
	$\mathit{fp}$ for \emph{Penguins do not fly}.
	$\Delta = \set{\mathit{fb}, \mathit{bT},\allowbreak \mathit{fb} \land \mathit{bT} → \mathit{fT}, \mathit{bp}, ¬\mathit{fp},\allowbreak \mathit{bp} \land ¬\mathit{fp} → ¬\mathit{fb}}$.
	The undercut tree arguing for $\mathit{fT}$ has root $\set{\mathit{fb},\allowbreak \mathit{bT},\allowbreak \mathit{fb} \land \mathit{bT} → \mathit{fT}}$, having as child $\set{\mathit{bp},\allowbreak ¬\mathit{fp},\allowbreak \mathit{bp} \land ¬\mathit{fp} → ¬\mathit{fb}}$.
\end{example}
In the interest of conceptual simplicity, the model can be summarized as follows.
\begin{example}[Short Tweety]
	\label{ex:shortTweety}
	Our language uses the atoms 
	$\mathit{fb}$ for \emph{Birds fly}; 
	$\mathit{fT}$ for \emph{Tweety flies}; 
	$\Delta = \set{\mathit{fb},\allowbreak \mathit{fb} → \mathit{fT}, ¬\mathit{fb}}$.
	The undercut tree arguing for $\mathit{fT}$ has root $\set{\mathit{fb},\allowbreak \mathit{fb} → \mathit{fT}}$, having as child $\set{¬\mathit{fb}}$.
\end{example}

This model has the same structure as \cref{ex:ball1}, but now it feels like the conclusion, $\mathit{fT}$, should not be warranted. (Our simplification of the model has no bearing on this discussion as \cref{ex:ball1} could be made more complex in order to reflect the structure of \cref{ex:fullTweety} instead.) However, another model is logically possible.
\begin{example}[Saving Tweety]
	\label{ex:savingTweety}
	We here assume that the speaker is a bird expert, talking about observations on an isolated island on which indeed every birds fly (there are no penguins on that island). It is therefore reasonable to think that she meant that birds fly in that context, not in general. Accordingly, we can include the atom $\mathit{ii}$, standing for \emph{“We are on an isolated island where no penguins (or other non-flying birds) live”}, and make the argument for $\mathit{fT}$ warranted by following the approach of \cref{ex:ball2}.
\end{example}

This again shows that, in the classical logic approach, the modeler needs to take a position on which arguments are implicit, which hurts neutrality, as this may determine the positions that will seem to be well supported as a conclusion of the analysis. Our claim is not that no reasonable ground ever exist to choose one model rather than another one. Indeed, practitioners routinely make such choice with common sense, based on what is known about the context of the discussion, which claims are plausible, and what the protagonists may reasonably be considered to mean. In the Tweety example, unless strong hints indicate that we face a particular context such as the one of \cref{ex:savingTweety}, it is clear that \cref{ex:shortTweety} is the reasonable model. Our claim is simply that there is no purely logical ground to choose one model rather than another one that would be easily summarized in a few rules. The model that will be adopted for any non-trivial debate is a subtle mix of the content of the debate and of hypothesis about the world that are in the modeler’s mind. Because there is no publicly known algorithm to obtain a model from arguments in natural language form, it is impossible to follow a systematic procedure to determine if the model is indeed reasonable. Trusting that the model is reasonable requires trust in the modeler, in the classical approach.

There is a need for an approach that makes the correctness of the attack relations a scientific (meaning, here, a precise and falsifiable) claim.

More generally, whenever an argument can be built using formulas from $\Delta$ such that $\Phi \cup \Psi ⊢ ⊥$, with coherent $\Phi, \Psi$, then it may be considered that a person holding that $\Phi$ could answer to the counter-argument $\Psi$ that, indeed, $\Psi$ is incompatible with $\Phi$, but that this is because $\Psi$ is false. This merely uses the well-known principle that an implication always has a corresponding contrapositive.

The following theorem shows that the problem illustrated here is not restricted to structures similar to the ones we chose as examples: as long as there is disagreement about some issue $\phi$ in a set of propositions $\Delta$, it is possible, by choosing to include or not a supplementary argument in favor of something that can be deduced already from $\Delta$, to make $\phi$ warranted or unwarranted.

\begin{definition}[Open-minded operator]
	For any $ \Psi, \Phi \subseteq \Delta$ such that $\Delta$ contains an unwarranted tree rooted at $\Phi$ arguing for $\phi$ with $\Psi$ as canonical undercutter, include a formula $\Phi → ¬\Psi$ in $O(\Delta) = \Delta'$. 
\end{definition}

\begin{theorem}
	For all propositions that are controversial ($\Delta$ permits to deduce both $\phi$ and $¬\phi$), $\phi$ is not warranted in $\Delta$ or in $O(\Delta) = \Delta'$.
\end{theorem}
Note that we merely assume that $\Delta'$ includes arguments whose entailments were already included in $\Delta$.

We conclude this section with a short note about the condition of non-repetition of the authors. Undercut trees are defined in such a way as to avoid repeating counter-arguments again and again, saying $\phi$, then as a counter argument, $¬\phi$, then as a counter-counter-argument $\phi$ again, and so on indefinitely. The technical trick used is to forbid a canonical undercutter of some node to be included in a tree if it consists solely of formulas that have been used already in the parents of that node (or the node itself). This rule makes sense to avoid infinite trees, and may also apparently solve the problem we highlight…

TODO: rather use $x$ attacked by $y → ¬x$ and then an implicit counter-argument $¬y$ (which can be used because either the first one disagrees that $y → ¬x$ or can logically claim from $x$ that $¬y$). This avoid the problem that it feels like repeating the argument. About $x$ attacked by $¬x$: need not say anything about this as it is already symmetric.

\subsection{General study}
Let $T$ be a set of possible texts in natural language and $p$ a procedure that transforms any text in $T$ into a set of undercut trees $\mathcal{U}$. Considering the classical logic approach first, we assume $p$ takes the following form: it first applies a set of possible interpretations of the text using a function $i$ to yield $\Delta, \Delta', …$, which are then used to produce undercut trees. Define $C$ as the classical logic approach defined above; applying $C$ to $i(t)$ thus results in a set of sets of undercut trees (one set for each $\Delta$).

For the conclusions to be non-trivial, we request that $\exists t \in T, \phi \in L(i(t)) \suchthat i(t) $ permits to deduce $\phi$ but $C(i(t))$ contains no tree with an undefeated root arguing for $\phi$. Here $L(i(t))$ denotes the set of formulas that can be formed using the atoms in $\cup i(t)$.

For the interpretations to be neutral (in the sense of making no assumption on the quality or content of the arguments in the text), we request that $i$ satisfies the following: $\forall t, \Delta \in i(t)$, $\Delta$ is closed under $¬¬$ (thus $\Phi \in \Delta ⇒ ¬¬\Phi \in \Delta$).
\footnote{Previously, but probably incorrectly, I requested for all $t$ that if $\exists \Delta \in i(t), \Phi \subseteq \Delta \suchthat MUS(\Phi)$, then $\exists \phi \in \Phi, \phi' \notin \Phi \suchthat MUS(\Phi \setminus \set{\phi} \cup \set{\phi'})$. $MUS(\Phi)$ means that $\Phi$ is a minimal unsatisfiable set.}

\begin{theorem}
	There exists no $p = C \circ i$ such that $i$ is neutral and $p$ has non-trivial conclusions.
\end{theorem}
Given a tree $(n, s)$ with $N = s[n]$ and $M \subseteq N$, define $D(M) = \set{S \subseteq N \suchthat s(M) \subseteq s^{-1}(S)}$ as the set of coalitions that attack all the attackers of $M$ (such a coalition is said to defend $M$). Say that $M$ is warranted iff for some finite sequence of the form $D_1 \in D(M), D_2 \in D(D_1), …, D_k \in D(D_{k - 1})$, we have $s(D_k) = \emptyset$.
\begin{lemma}
	A root is undefeated iff the singleton containing the root is warranted.
\end{lemma}

\subsection{More discussion}
\commentOCf{Contenu de cette section à reformuler ou effacer à terme…}

The assumption we want to discuss here is the following.
Many articles start with a postulated known attack relation or logic representation of arguments. It is natural to wonder where this information might come from, and we suppose, although this is seldom discussed explicitly, that the assumption is that it is more or less easy to obtain such relations from the natural language form in which arguments generally arise in the wild. 
The postulate that we want to argue against here is that by inspecting the arguments and using common sense, either the attack relation can be seen immediately, or a logical representation of the arguments can unambigiously be obtained, itself yielding a non-trivial attack structure by logical manipulation.
By non-trivial, we mean here that the structures that are obtained do permit, at least sometimes, to discriminate between a claim and its negation, even though both the claim and its negation can be deduced from arguments in the text. (Intuitively, we want to exclude cases of obtaining entirely symmetric conclusions.)

Accordingly, we focus on the case of arguments given under natural language. 

We make the following hypothesis of robustness against interpretion. The set of possible interpretations is $I$. If an interpretation leads to $\Delta$, and some argument in the DB says $\Phi$ and is attacked by $\Psi$, and it is possible to deduce, from the DB, using elements from $\Phi$ and $\Psi$, a counter-argument, then it is included in the set of interpretations.

Undoubtedly, some situations let one determine easily the attack relation, by simple appeal to an obvious consensus. In the classical textbook example of Alice saying that “it will rain tomorrow because the weather forecast on the BBC said so” and Bob replying that “it will not rain tomorrow because the weather forecast on Channel 4 said so”, the situation is clear, and a simple appeal to consensus permits to solve any doubt to the contrary: give these arguments to any two persons, and they will agree that these arguments attack each other. Such trivial situations, although being possibly adequate for illustrative purposes of technical definitions, are however hardly representative of the complexity of the debates that an argumentation framework must be able to tackle. It seems reasonable to demand of argumentation theory to help us think about debates involving subtle, complicated arguments. For example, a framework could claim to be able to help a medical doctor take a decision about which treatment to prescribe, or help an individual to choose which political party she will vote for.

Our main claim is that there is no neutral formal way to determine an attack relation that is not symmetric. In other words, obtaining a non-symmetric attack relation requires the analyst to adopt a position in the debate, in the sense of considering some claims as “winning” over others in a way that cannot be justified in purely formal terms.

First, note that one can always interpret “kindly” any argument, by filling in (considering as implicit) any supplementary claim that is required to make it work. To illustrate, assume John says: “because the apple detached from the tree, it fell on the ground”. The antecedent is not sufficient to bring the consequent: in case of storm, for example, the apple could detach from the tree and (temporarily) not fall on the ground. Depending on the circumstances of the speech, however, it might be perfectly reasonable to assume that John really meant: “because the apple detached from the tree and there was no storm on that day, it fell on the ground”. Indeed, if a required element in a reasoning is missing, it may be because it is an implicit assumption (that perhaps John knows is shared by the audience, hence does not consider useful to say). Now, interpreting kindly any argument will constantly lead to abstain from any conclusion: any argument that attacks any other will be attacked in return. Indeed, imagine someone says: “as there was storm on that day, the apple did in fact not fell on the ground”. Interpreting John’s text kindly, we would conclude that these arguments attack each other, and will not be able to conclude anything.

If we want to avoid this unfortunate consequence of extreme permissivism, we must restrict which attacks we (as modelers) we consider as “plausible”. In the famous Tweety example, we consider likely that the defeasible rule Bird(x) => Fly(x) is abusively used for building the argument that Bird(Tweety) and therefore Fly(Tweety), in a case where Tweety is a penguin; and we obtain a not too permissive model where the argument Penguin(Tweety) attacks the first one. But we could, again, conclude otherwise: if, unknown to the modeler, the person uttering the sentence in favor of Tweety flying is talking about an island where there are only birds that fly, then the use of the rule is authorized, and the person saying that Tweety is a Penguin must be mistaken. It is a choice of the modeler to conclude otherwise.

This shows that one will either be overly permissive, or need a trusted source of knowledge (that defines a hierarchy of claims by plausibility, say).

Another way to view this tension is to observe that we need to interpret what John says: we do not know what is missing from his reasoning because he did not realize some supplementary hypothesis was required (and therefore possibly obtained a mistaken conclusion), or because he thought it was sufficiently clear for the audience that the hypothesis was holding in the context of his speech.

More generally, using the scheme requires to add a priori knowledge, which is usually left implicit in the literature but is nonetheless required. Defeasible rule: A implies B. Explicit rule: A and X imply B. Attack: not X. But we might instead make the first speaker win the argument by instead being more kind to him and considering that the mere fact that he used the rule A implies B means that he implicitly thought it clear that X hold as well. (This kind hypothesis may seem reasonable in some contexts, such as when the speaker is an expert in the relevant domain.) Also, it could be that there is another way to complete the defeasible rule A implies B, by saying A and Y imply B. In that case, not(X) is uneffective against the first speaker. When the modeler considers that an argument attacks another one, and not conversely, she is inevitably taking a (possibly implicit) position in favor of some of the arguments.

We can rephrase this by saying that a first and important reason to not consider the attack relation known a priori is that considering it known a priori seems to require to assume that the attack relation is objective, thus in particular, that whether an argument attacks another one does not depend on who is listening, or on a choice of interpretation. The objectivity assumption permits to declare that an argument “truly” attacks another one, even though some given person may fail to realize it or may even believe the contrary. 

Real-life debates feature omissions and implicit statements. It may be fundamentally undetermined (and not simply unknown) which statements exactly have been omitted so as to connect an argument to what the debater has said previously. The debater himself may be hard pressed to explain it, if asked: we human being may not always form fully precise statements mentally before uttering them, or if we do, we do not necessarily have access to the precise version of the statements. A mathematician might “feel” that a proposition is true long before knowing how to prove it. A person may intuitively “see” how one argument supports or attacks another one without being able to fill in all the gaps and make it a precise reasoning. Or there may be multiple reasonings that allow to fill in the gaps, and the debater may erroneously think that they are just different ways of expressing the same idea; whereas it could be that one phrasing features a mistake in reasoning whereas another one does not. It is the norm with philosophers debating of complex ideas to consider multiple versions of their opponent’s claim, trying to interpret them “kindly”, that is, filling what they see as gaps in their opponent’s reasoning in ways that make their opponent’s view point as solid as possible (cite Denett). It seems hard to justify that one of these versions in particular would objectively be the right version. And if there are indeed multiple interpretations of a statement that correspond to multiple precise arguments, it seems unlikely that they all would lead to the same attack relation when considered within a set of related arguments. Therefore, in some interesting cases, the attack relation is not objectively determined.

Another important reason to not consider the attack relation known a priori relates to the understanding of the attack relation itself. 
Even when arguably “what the debater means objectively” is well defined, one may, in some applications, be interested in what some given listener understands from the debate, rather than by what the debater means. 
This is an important distinction in our context because the attack relation representing what the listener understands may differ from the one representing what the debater means.
The listener may not know the meaning of some words used by the debaters, or she may understand them differently than what the debaters have in mind. 
The listener may not have the deductive power to understand what could otherwise be considered a trivial implication of the argument. 
The listener may lack some required knowledge.

Translating natural language arguments into a logic language does not constitute a solution to these problems, as the problems exist, unchanged, at the level of translation. When there are multiple ways of interpreting some implicit information present in an argument (as intended by the debater, or as understood by the listener), the modeler must choose one when translating it in a formal, precise language, with the risk of introducing an arbitrary choice or losing neutrality.

A practical argument is that encoding the arguments logically require important expertise, which may be unavailable or cost too much.

\commentOC{To discuss: which points require examples? How complex (toy examples VS realistic examples)? Consider an example of a logic encoding with different conclusions?}

Practical studies showed that rejected arguments are not necessarily to be interpreted as non-receivable, but rather as rough ideas that need to be refined \cite{EJDP2018, Bourguet2013} and can serve as initial pillars of great importance for the construction of the argumentation system. Talk about obtaining judgements of individuals, …

\section{what if not known a priori?}
We just argued that it may be relevant or convenient to not assume we know the attack relation. In this section we provide some ideas of subject of studies that do not require this assumption, that pertains to computer science, and does not require advanced natural language processing capabilities.

(To be continued.)
We consider that what counts is what the receiver of the argument says and we propose to study several aspects related to this perspective: how to find decisive arguments in this sense; is this consensual; what are appropriate methods, statistical or others; what are the protocols that must be used to gather arguments; how can we test that every arguments that are possibly relevant have been considered?

\section{a sketch of a possible approach}
Let people come to a website and argue and indicate which arguments theirs answer, then confront this to the opinion of other visitors
\commentRT{Raccrocher Nicolas ici ?}

\section{Others}
We should decide whether to include some of these points in the text or adding sections about them or dropping them.
\begin{itemize}
	\item possibly, contrast with persuasion?
	\item link to explainable AI (should focus on decisive arguments), exploitation of weaknesses of will or sub-conscious behavioral patterns (youtube autoplay; nudge; anchoring) VS reflective arguments (advocacy), and the like? Link to deliberative democracy?
	\item review existing experimental work in FAT? Other related works? Philosophical approaches (Rawls, Habermas)?
\end{itemize}

\bibliography{empirical}

\end{document}
