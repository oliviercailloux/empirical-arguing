\RequirePackage[l2tabu, orthodox]{nag}
\documentclass[version=3.21, pagesize, twoside=off, bibliography=totoc, DIV=calc, fontsize=12pt, a4paper, french, english]{scrartcl}
\input{preamble/packages}
\input{preamble/redac}
\input{preamble/math_basics}
\input{preamble/math_mine}


\usepackage{silence}
\WarningsOff[natbib]


%I find these settings useful in draft mode. Should be removed for final versions.
	%Which line breaks are chosen: accept worse lines, therefore reducing risk of overfull lines. Default = 200.
		\tolerance=2000
	%Accept overfull hbox up to...
		\hfuzz=2cm
	%Reduces verbosity about the bad line breaks.
		\hbadness 5000
	%Reduces verbosity about the underful vboxes.
		\vbadness=1300

\title{Study argument attacks empirically}
\author{Olivier Cailloux}
\affil{Université Paris-Dauphine, PSL Research University, CNRS, LAMSADE, 75016 PARIS, FRANCE\\
	\href{mailto:olivier.cailloux@dauphine.fr}{olivier.cailloux@dauphine.fr}
}
\author{Pierre Bisquert}
\affil{Affiliation}
\author{Nicolas Salliou}
\affil{Affiliation}
\author{Rallou Thomopoulos}
\affil{IATE, Univ Montpellier, INRAE, Institut Agro, MONTPELLIER, FRANCE\\
	\href{mailto:rallou.thomopoulos@inrae.fr}{rallou.thomopoulos@inrae.fr}
}
\hypersetup{
	pdfsubject={Argumentation},
	pdfkeywords={Position paper, Formal argumentation theory, Review, Logic-based argumentation},
}

\begin{document}
\maketitle

\section{Introduction}
\label{sec:intro}
Formal Argumentation Theory (FAT) \cite{Dung95} relies on “attacks” between arguments to compute most acceptable standpoints, in the form of the so-called “extensions”. These attacks are typically formalized as a binary relation considered as representing the directed contradiction or incompatibility status of pairs of arguments. In the vast majority of the articles in formal argumentation theory, the attacks are considered known, given to the analyst as a starting point of the study. 

In this position paper, we want to argue in favor of extending the scope of formal argumentation theory by relaxing this assumption. In particular, we argue that attacks should not necessarily be considered as representing an objective status of contradiction, as given for example by inspection of logical representation of arguments. We will give methodological and practical reasons to consider the attack relation as representing the subjective state of mind of an individual; and to study methods for obtaining information about this relation empirically.

Although some authors seem to acknowledge incidentally that attack relations sometimes can’t be considered known (cite Besnard \& Hunter), as we will see, few works in FAT avoid this assumption. One may think of at least two reasons for this apparent gap. First, to the best of our knowledge, the reasons for doubting this assumption have not been analyzed clearly. Second, there might be a lack of an idea of where to start if one relaxes this assumption, perhaps fearing that these questions would reveal hopelessly complex and that would be of interest only to the specific field of natural language processing. We provide here (partial) answers to these two points: we give reasons for doubting this assumption; and we draw paths to study attack relations considered subjective that do not relate to natural language processing.

Obtaining a formal argumentation model out of debates expressed in natural language is useful in various circumstances. Two goals that we have in mind are, first, when the modeler wants to faithfully reflect the protagonists’ opinions; second, when the modeler wants to help individuals not taking part in the exchange of arguments to form a deliberated opinion about a topic. 

Our first point is that formal models do not generally rest on logic alone. It is necessary to use judgment about the quality of the arguments and the implicit claims, those that can reasonably be assumed to be held by some protagonists without having been expressly uttered in the debate. We suspect many readers will readily acknowledge that this claim is true, but considering that this is seldom discussed explicitly in the FAT literature, we will provide examples to explain why this must be considered to hold.
This threatens the neutrality of the modeler, as judgments about quality of arguments or about what can reasonably be considered to be implicit arguments in a debate rest, in all but the most trivial cases, on a vast amount of unformalized assumptions about the world and about how human beings think; and making all of them explicit is generally out of reach in practical real-world cases.

One may declare this to not be a problem: the modeler could use “common sense” in modeling the arguments, and postulate that other modelers would reach the same decisions, thus, that their common sense leads to the same model as anyone else’s. In the positive cases, the modeler applies good faith, has “good common sense”, and is not biased towards one side of the debate. We summarize these attributes by saying that the modeler is neutral.
Practitioners can use such neutrality to obtain models, as our examples will illustrate, and if neutrality holds, the validity of the resulting model can be considered good even though the model rests on unformalized assumptions about the world. This requires, however, to trust that the modeler is neutral.
 
It is sometimes important to not require trust in the modeler. Consider for example an application where a modeler is paid by a social network company to summarize arguments in order to help participants see more clearly faulty reasoning, in an attempt to decrease the speed at which false news spread on such networks.
Trusting blindly the modeler in such a case can be undesirable, as it may give an undesired high power to the company.
Similarly, when a public authority organizes public debates about some social matter, it is desirable to not have to trust the modeler.
In such examples, it is required but not sufficient that the modeler be neutral. The well known phrase, that justice must not only be done, but must also be seen to be done, applies here. In order to build and legitimate public trust, one must be able to verify that the modeler is neutral. This article therefore proposes to build a procedure that allows principled verifications to be done in order to relax the requirement of neutrality on the modeler.

One way to ensure neutrality may be to not take position at all. Whenever there is a disagreement, just record the fact that the disagreement exist, and take no position on the possible results. This approach fails to accomplish the goals we are interested in here, of helping third-parties to form a deliberated judgment or of recording the debater’s opinions to present them truthfully to an audience. In many practical cases, judgment is required to summarize the information in a useful way. For example, some arguments may be redundant (express the same point in different ways); some arguments may be weaker than other ones. We will present a strategy that permits to take such differences into account in a model, without requiring a complete account of how to determine the strengths of various arguments, but by providing a means to check whether the resulting model is adequate.

Preference based approaches still considers an attack relation, and the preference is supplementary data on top of the attack relation. (TODO: check Besnard \& Hunter - Elements of Argumentation, Chapter 6: Considering the Audience.) \commentRT{Transition sur preference-based approaches \`a expliquer car tombe comme un cheveu sur la soupe. 1) Subjective = related to a given audience, 2) Audiences introduced in (contextual) preference-based frameworks, 3) Cite different works (PAF, VAF ...)}

We want to distinguish between two attitudes towards the attack relations. 

The most usual attitude is what we call the \emph{a priori} attitude, reviewed in this section: the attack relations are considered known. They are deducible using a systematic procedure from the arguments. This covers several cases:
\begin{itemize}
\item the case where the arguments are considered given under a logic form and this is used to deduce the attack relations;
\item the case where the input itself is considered to be the attack relation;
\item and the case where the input is considered a set of arguments in natural language (NL), and the attack relations are deduced from these NL arguments. (One can think of text form, video, …)
\end{itemize}

Another attitude consists in considering the attack relations as empirical. Under that view, the attacks are revealed by something else than the arguments themselves and a priori knowledge, such as the reactions of individuals to the arguments, as we will propose.

Mention the study of \cite{ICCS2018} on comparisons of attacks? \commentRT{Faut-il ajouter les id\'ees suivantes : dans Yun et al. 2018 (ICCS) \`a partir d'un m\^eme ensemble d'arguments initial, 3 proc\'edures syst\'ematiques sont tour \`a tour appliqu\'ees pour d\'efinir les attaques ; puis les extensions sont calcul\'ees ; enfin les r\'esultats des 3 cas sont \'evalu\'es empiriquement par plusieurs groupes. L'objectif \'etant d'\'evaluer l'intuitivit\'e des 3 d\'efinitions.}

Besnard \& Hunter (Chapter 3, p. 38 to 39): “Note that we do not assume any metalevel information about formulae. In particular, we do not assume some preference ordering or ‘‘certainty ordering’’ over formulae. This is in contrast to numerous proposals for argumentation that do assume some form of ordering over formulae. Such orderings can be useful to resolve conflicts by, for example, selecting formulae from a more reliable source. However, this, in a sense, pushes the problem of dealing with conflicting information to one of finding and using orderings over formulae, and as such raises further questions such as the following: Where does the knowledge about reliability of the sources come from? How can it be assessed? How can it be validated?” \commentRT{Qu'est-ce qu'on conclut de cette citation ? Que Besnard \& Hunter ne supposent pas l'existence de pr\'ef\'erences ? Qu'ils n'envisagent pas l'attaque subjective ?}
In Chapter 4, they argue for an enlarged view. Selectivity is important. Impractical to represent every argument; some arguments are similar and can be summarized; readers do not want or can’t read many arguments and want a summary. They give multiple examples of situations where it is important to take into account the audience attitude towards the presented arguments.

\section{The usual perspective (in FAT?) about attacks}
Here we describe succinctly the main paradigms that have been proposed in argumentation to model arguments and attack relations, focusing on the tactics used to obtain asymetric attacks (arguments such that one attack another one but not conversely).

Our main goal is to argue that a model cannot be obtained out of a text in natural language using a systematic procedure that requires no judgment on the quality of the arguments or about implicit arguments.
To discuss the contrary intuition, let us start with the famous example about Tweety.
\begin{example}
	Assume that it is consensual in some conversation that Tweety is a bird. Someone says that because Tweety is a bird, Tweety flies. Another person disagrees and says that penguins are birds but do not fly; therefore, we cannot conclude that Tweety flies.
\end{example}
This example is interesting because it may suggest that it is possible to obtain a useful and formal model by applying some systematic procedure, mapping sentences in natural language to, e.g., sentences in a formal language. 

We want to convey reasons to believe that such approach is generally mislead, if it does not grant that unformalized judgment is required while building the model.
Far too many procedures and formal languages have been proposed for examining them all, but applying the discussion on a few important paradigms should suffice to realize that the problem is fundamental, and not related to the specificities of a given formal language or a given procedure.

We will in this section review important classical proposals in obtaining formal argumentation models, and then illustrate that they do require judgment on the modeler’s part.

\subsection{Classical logic}
In this section we consider the proposition described in the famous book by \citet{besnard_elements_2000} about argumentation on the basis of classical logic. We focus on their concept of trees, as it permits to determine which propositions are warranted. We simplify some of their definitions, for example, by defining trees over sets of formulas instead of over arguments, and show in footnotes the original definitions so that the reader can observe that our definitions are equivalent to the original propositions.

Let $\allformulas$ be an (infinite) set of possible formulas in predicate logic with the usual logical connectives $\land$, $\lor$, $¬$, $→$ and the usual deduction operator $⊢$. 
We use $⊥$ for “falsity”, as is classical, thus, for example, $\Phi \cup \Psi ⊢ ⊥$ denote that the formulas in $\Phi$ and $\Psi$ cannot all be satisfied. A set of formulas $\Phi$ is \emph{consistent} iff $\Phi ⊬ ⊥$.

Let $\Delta \subseteq \allformulas$ be a finite set of formulas.
An argument is a pair $(\Phi, \alpha)$ with $\Phi \subseteq \Delta$, called the support, and $\alpha \in \allformulas$, called the claim, such that $\Phi$ is a consistent minimal subset of $\Delta$ satisfying $\Phi ⊢ \alpha$ (thus $\Phi ⊬ ⊥$, $\Phi ⊢ \alpha$ and $\forall \Phi' \subset \Phi: \Phi' ⊬ \alpha$).
Let $\allargs$ denote the set of all arguments.

Given two sets of formulas $\Phi, \Psi \subseteq \Delta$, say that $\Psi$ is a canonical undercut for $\Phi$ iff $\Psi$ is a consistent minimal subset of $\Delta$ satisfying $\Phi \cup \Psi ⊢ ⊥$ (thus $\Psi ⊬ ⊥, \Phi \cup \Psi ⊢ ⊥$ and $\forall \Psi' \subset \Psi: \Phi \cup \Psi' ⊬ ⊥$).
\footnote{The authors define an undercut for an argument $(Φ, α)$ as an argument $(Ψ , ¬(φ_1 ∧\ldots ∧ φ_n))$ where $\set{φ_1, \ldots, φ_n} ⊆ Φ$, and define that an argument $(Ψ , ¬(φ_1 ∧ \ldots ∧ φ_n))$ is a canonical undercut for $(Φ, α)$ iff it is an undercut for $(Φ, α)$ and $(φ_1, \ldots, φ_n)$ is the canonical enumeration of $Φ$ (a canonical enumeration of every $\Phi \subseteq \Delta$ is supposedly given).
We define this concept as a relation over the sets of formulas rather than on arguments to make it clear that canonical undercuts depend only on the supports of the arguments.}

\begin{example}
	\label{ex:abstract}
	Consider $\Delta = \set{x, y, y → ¬x, z, z → ¬y}$. Examples of arguments include $(\set{x}, x)$, $(\set{y, y → ¬x}, ¬x)$ and $(\set{z, z → ¬y}, ¬y)$. 
	The only canonical undercut for $\set{x}$ is the set $\set{y, y → ¬x}$. 
	The canonical undercuts for $\set{y}$ are the sets $\set{z, z → ¬y}$ and $\set{x, y → ¬x}$. 
	The only canonical undercut for $\set{y → ¬x}$ is the set $\set{x, y}$. 
	The canonical undercuts for $\set{y, y → ¬x}$ are $\set{z, z → ¬y}$ and $\set{x}$.
\end{example}

The authors propose to use the status of the root nodes of argument trees as one way of discriminating arguments. Let us define undercut trees with this goal in mind.

Define an argumentation path of length $k$ as a finite sequence $p \in \powerset{\Delta}^k$ of subsets of $\Delta$ such that every subset of formulas undercuts the previous one (thus $\forall 1 ≤ i ≤ k - 1$: $p_{i + 1}$ undercuts $p_i$) and every subset of formulas contain some new formulas compared to the ones used before (thus $\forall 1 ≤ i ≤ k - 1$: $p_{i + 1} \nsubseteq \cup_{l ≤ i} p_l$).

Canonical undercuts permit to associate to any set of formulas $\Phi \subseteq \Delta$ the undercut tree $u_\Phi$, defined as a tree of argumentation paths, whose root is the path $(\Phi)$ of length $1$, and whose children are defined recursively as follows. Given a node $p$ of length $k$ in the tree, its children $u_\Phi(p)$ are all the argumentation paths of length $k + 1$ that extend $p$ (thus, the paths $p'$ such that $\forall i ≤ k: p'_i = p_i$, such that $p'_{k + 1}$ undercuts $p_k$ and such that $p'_{k + 1}$ has some formula not used in $p$).

We display the nodes in undercut trees using only their last element, thus, using a single set of formulas. (It is however required in the general case to define formally nodes as more complex objects and not merely as sets of formulas, in order to guarantee that the resulting structures are trees, thus, that a node has only one parent.)

Say that an undercut tree rooted at $\Phi$ argues for $\alpha \in \allformulas$ iff $(\Phi, \alpha)$ is an argument.
\footnote{The authors define an argument tree for $\alpha$ as a tree of arguments, rooted at an argument $a_1 = (\Phi_1, \alpha)$, such that the children of a node $a = (\Phi, \beta)$ consist of all canonical undercuts $(\Psi, \gamma)$ for $a$ such that $\Psi$ is not a subset of $\Phi \cup \Phi_1 \cup \Phi_2 \cup … \cup \Phi_n$, where $(Φ_1, \beta_1), …, (Φ_n, \beta_n)$ denote the ancestors of $(\Phi, \beta)$.

As the authors observe, it follows that an argument tree arguing for $\alpha$ has all of its arguments (except for the root) claiming the negation of the support of its parent. We neglect claims and focus on supports in the nodes of our trees in order to reflect formally the fact that claims bring no information (apart from the one at the root).

Our definition corresponds to theirs for the following reason.
An argumentation path in an undercut tree is uniquely defined by its last two elements, because two siblings $p, p'$ (nodes at a given level $k$) in such a tree have different last elements ($p_k ≠ p'_k$), as is easily seen by induction on the depth of the tree (it follows from that observation that if two paths have the same last elements, they are not siblings, thus, have a different parent).
We can thus also view any undercut tree $u$ rooted at $\Phi$ arguing for $\alpha$ as a tree defined over arguments, having the same topology as $u$, having $(\Phi, \alpha)$ as root, and having in place of each argumentation path $p$ of length $k ≥ 2$ in $u$ the argument $(p_k, ¬p_{k - 1})$.
}

\begin{figure}
	\caption{The undercut tree arguing for $x$}
	\label{fig:utx}
	\begin{tikzpicture}
		\tikzset{every node/.style={draw, ellipse, execute at begin node=$,execute at end node=$}}
		\path node (Phi1) {\Phi_1 = \set{x}};
		\path (Phi1) ++(0, -2cm) node (Phi2) {\Phi_2 = \set{y, y → ¬x}};
		\path (Phi2) ++(0, -2cm) node (Phi3) {\Phi_3 = \set{z, z → ¬y}};
		\path[draw, <-] (Phi1) -- (Phi2);
		\path[draw, <-] (Phi2) -- (Phi3);
	\end{tikzpicture}
\end{figure}
\begin{example}[Continuing \cref{ex:abstract}]
	\label{ex:abstUnder}
	\Cref{fig:utx} shows the sole undercut tree arguing for $x$ in our running example. There is only one such tree as the only argument that can be built for $x$ from $\Delta$ is $(\set{x}, x)$. Accordingly, \cref{fig:utx} shows the tree $u_{\set{x}}$, with root formally defined as the argumentation path $p^{(1)} = (\Phi_1)$ of length $1$ but displayed simply as the element $\Phi_1$. 
	The set of children $u_{\set{x}}(p^{(1)})$ of the root is the singleton constituted by the argumentation path extending $p^{(1)}$ using the only canonical undercut of $\set{x}$, that is, the argumentation path $p^{(2)} = (\Phi_1, \Phi_2)$, with $\Phi_2 = \set{y, y → ¬x}$, of which we display only the last element.
	To compute the children of this node, $u_{\set{x}}(p^{(2)})$, observe that out of the two canonical undercuts for $\Phi_2$, only one uses some new formulas (that is, formulas that are not in $\Phi_1 \cup \Phi_2$). Thus, the unique permissible element for extending the path $p^{(2)}$ is $\Phi_3 = \set{z, z → ¬y}$, yielding $u_{\set{x}}(p^{(2)}) = \set{p^{(3)}}$ with $p^{(3)} = (\Phi_1, \Phi_2, \Phi_3)$. 
	Finally, $u_{\set{x}}(p^{(3)}) = \emptyset$ as no new formulas exist at this stage: $\cup_{l ≤ 3} p^{(3)}_l = \Phi_1 \cup \Phi_2 \cup \Phi_3 = \Delta$.
	
	The unique undercut tree arguing for $¬x$, displayed in \cref{fig:utnx}, is $u_{\Phi_2}$ with $u_{\Phi_2}((\Phi_2)) = \set{(\Phi_2, \Phi_1), (\Phi_2, \Phi_3)}$ and $u_{\Phi_2}((\Phi_2, \Phi_1)) = u_{\Phi_2}((\Phi_2, \Phi_3)) = \emptyset$.
\end{example}
\begin{figure}
	\caption{The undercut tree arguing for $¬x$}
	\label{fig:utnx}
	\begin{tikzpicture}
		\tikzset{every node/.style={draw, ellipse, execute at begin node=$,execute at end node=$}}
		\path node (Phi2) {\Phi_2 = \set{y, y → ¬x}};
		\path (Phi2) ++(-2cm, -2cm) node (Phi1) {\Phi_1 = \set{x}};
		\path (Phi2) ++(2cm, -2cm) node (Phi3) {\Phi_3 = \set{z, z → ¬y}};
		\path[draw, <-] (Phi2) -- (Phi1);
		\path[draw, <-] (Phi2) -- (Phi3);
	\end{tikzpicture}
\end{figure}

Given a tree $u_\Phi$, define the set $R$ of defeated nodes as the subset of nodes such that $p \in R$ iff $u_\Phi(p) \nsubseteq R$, thus, if some successor of $p$ in $u_\Phi$ is not in $R$.
One can see that this defines a unique set by observing that the definition determines the status of each node starting from the leaves up to the root: the leaves are undefeated, the predecessors of some leaf are defeated, and so on.
We say that a formula $\phi \in \allformulas$ is \emph{warranted} iff some undercut tree arguing for $\phi$ has an undefeated root.
\begin{example}[Continuing \cref{ex:abstUnder}]
	The formula $x$ is warranted: the leaf $\Phi_3$ of the tree $u_{\set{x}}$ is undefeated, thus its parent $\Phi_2$ is defeated, hence, the root $\Phi_1$ is undefeated (we designate nodes by their last elements only as this creates no ambiguity here). The formula $¬x$ is not warranted, as the root of the unique tree $u_{\Phi_2}$ arguing for $¬x$ is defeated.
\end{example}

\subsection{PAFs}
\label{sec:pafs}
Here we talk about Preference-based Argumentation Frameworks.

Considering the book Rahwan, Simari - Argumentation in Artificial Intelligence (2009), Chapter 15: Argumentation for Decision Making, Amgoud.

We assume that there is only one alternative on which to decide: $\dy$ or $\dn$ (e.g., “surgery” or “do nothing”). This simplifies the formal exposition without removing the crucial points of the propositions.

A decision system is $A_e, A_p^{\dy}, A_p^{\dn}, {≥_e}, {≥_p}, R_e, R_m$. 
\footnote{I assume that ${≥_m} = A_e × A_p$. Her section 2.2 suggests ${≥_m} = A_e × A_p$, Example 15.5 suggests ${≥_m} \subseteq A_e × A_p$; I assume the mistake is in the example. Also, $Def_m = R_m$ and $Def_e = R_e \cap (\overline{>_e})^{-1}$. $(\overline{>_e})^{-1}$ contains the pairs $(a, b)$ such that $a$ is strictly preferred to $b$ or they are indifferent or incomparable. $Def = (R_e \cap (\overline{>_e})^{-1}) \cup R_m = R.$}
Define $A_p = A_p^ {\dy} \cup A_p^{\dn}$.
Define $\allargs = A_e \cup A_p$ as the set of all arguments.

${≥_e} \subseteq A_e × A_e$, ${≥_p} \subseteq A_p × A_p$, both reflexive and transitive, the preferences.

$R_e \subseteq A_e × A_e$, $R_m \subseteq A_e × A_p$, the “objective” attacks. 

From $R = (R_e \setminus {<_e}) \cup R_m$, we obtain the extensions (an extension is a set of arguments). The stable and the preferred semantics are considered. Then the skeptically accepted arguments are $\cap E_i$ and the credulously accepted ones are $\cup E_i \setminus \cap E_i$.
\footnote{More from Amgoud: skept pref subseteq skept stable if not empty.}

$E$-acceptable = $E$-defended = all args that $E$ defends = those whose attackers are all attacked by $E$ = $\set{a \suchthat R^{-1}(a)\subseteq R(E)}$.

Conflict-free: contains no arguments that attack each other.
\footnote{More from Baroni and Giacomin: Admissible $E$: conflict-free and all included are $E$-defended; Complete $E$: $E$ admissible and includes every $E$-defended; Preferred: maximal admissible set, equivalently, maximal complete extensions. Stable: conflict-free and $E$ attacks all excluded args. Implies complete.}

Complete $E$: $E$ conflict-free and $E$ = $E$-defended.

Stable: conflict-free and $E$ attacks all excluded args. Implies maximally complete.

Preferred: maximal complete. Stable implies preferred. 

The semantics considered is: the stable ones if some exist; the preferred ones otherwise. Define $S$ as the skeptically accepted arguments in the resulting semantics, thus, $\cap E_i$ with the $E_i$ sets being the stable ones, or, if there is none, the preferred ones.

Given a set $A \subseteq \allargs$, say that $A$ is possibly failing iff $\exists B \subseteq \allargs$ with $B$ admissible and attacking every member of $A$ ($A \subseteq R(B)$).
Inversely, $A$ is necessarily resistant iff it is not possibly failing.

Interesting examples. $(5, 3), (3, 2), (2, 4), (4, 3), (2, 1), (1, d), (4, d)$. Here, 5 invalidates 3, which “suffices” to “resolve” the cycle as $(3, 2)$ “does not count” any more. Thus, 2 defends $d$ and $d$ is skeptically accepted. Also, consider: $(5, 4), (4, 3), (3, 2), (2, 4), (3, 1), (4, 1), (1, d)$. Without 5, $d$ is not sceptically accepted because 1 may be considered to resist thanks to the undecided status of the cycle, but with 5, $d$ becomes sceptically accepted as 1 is defeated.

\subsection{Defeasible logic}
\commentOCf{TODO: formalisme de base de la logique defeasible. Pierre nous fournit un article ou livre de base s’il en trouve un qui semble adéquat. Thèse suggérée : \url{https://hal.inria.fr/tel-01904558v2}}

Consider a propositional language $\mathcal{L}_p$ with the implication and conjunction connectives ($\rightarrow$,$\Rightarrow$,$\curly$,$\land$) and strong negation ($\lnot$) on a finite set of literals where a literal is either an atomic proposition or its negation. 
The set of literals is denoted $F$.

The body and head of a rule are finite conjunction of literals. A rule expressed in $\mathcal{L}_p$ is applicable on a set of literals if its body is included in this set.
There are three kinds of rules:
\begin{itemize}
\item \emph{strict rules}, denoted $\Phi \rightarrow \varphi$, indicating definite implications; we denote by $R_s$ the set of strict rules;
\item \emph{defeasible rules}, denoted $\Phi \Rightarrow \varphi$, indicating plausible implications; we denote by $R_d$ the set of defeasible rules;
\item \emph{defeater rules}, denoted $\Phi \curly \varphi$, preventing defeasible implications; we denote by $R_{df}$ the set of defeater rules;
\end{itemize}
The set of all rules is denoted $R$, the conclusion of a rule $r$ is denoted $Head(r)$ and its body $Body(r)$.

In order to express higher plausibility for some defeasible implications, a (generally acyclic) preference relation $\succ$ on $R$ is needed; $r_1 \succ r_2$ indicates that $r_1$ is superior to $r_2$ (and $r_2$ inferior to $r_1$).

A defeasible theory $T$ is a triple $(F,R,\succ)$ which will assign a label to any literal $f$ according to its provability:
\commentOCf{Est-ce que toute théorie affecte un unique label à tout litéral ? (Ou est-ce qu’un litéral n’a parfois aucun label, ou que plusieurs labels peuvent s’appliquer ?)} 
\begin{itemize}
\item $+\Delta f$: $f$ is provable using only strict rules and literals in $T$,
\item $-\Delta f$: it is proved that $f$ is not strictly provable in $T$,
\item $+\delta f$: $f$ is provable using at least one defeasible rule or literals in $T$,
\item $-\delta f$: it is proved that $f$ is not deeasibly provable in $T$.
\end{itemize}
Here $+\Delta f$ represents a labeled literal, where $f$ is a literal and $+\Delta$ is its label.

The general idea of defeasible logics is then to provide a proof for a given conclusion (literal). A proof is a finite sequence $P = (P(1), \dots, P(n))$ of labeled literals ($P(1..i)$ denotes the initial part of the proof P of length $i$) satisfying the following conditions \cite{Billington1993}:
\commentOCf{Un litéral a un label donné ssi il existe une preuve qui se termine par ce litéral avec ce label ?}
%je comprends correctement que tout litéral est soit $+ \Delta$, soit $- \Delta$ ? Donc soit $f$ est prouvable strictement, soit il est prouvable qu’il n’est pas prouvable strictement ? Et même chose pour $+ \Delta$ VS $- \Delta$, n’est-ce pas ?}

\begin{itemize}
\item $+\Delta$: if $P(i+1) = +\Delta f$ then either
  \begin{enumerate}
  \item $f \in F$ or
  \item $\exists r \in R_s$ s.t. $f \in Head(r)$ and $\forall \varphi \in Body(r): +\Delta \varphi \in P(1..i)$.
  \end{enumerate}

\item $-\Delta$: if $P(i+1) = -\Delta f$ then
  \begin{enumerate}
  \item $f \notin F$ and
  \item $\forall r \in R_s$ s.t. $f \in Head(r)$, $\exists \varphi \in Body(r): -\Delta \varphi \in P(1..i)$.
  \end{enumerate}

\item $+\delta$: if $P(i+1) = +\delta f$ then either
  \begin{enumerate}
  \item $+\Delta f \in P(1..i)$ or
  \commentOCf{Il me semble qu’on peut avoir $+\Delta f$ et $+ \delta f$ : avec $f \in F$, $P(1) = +\Delta f$, $P(2) = +\delta  f$.}
  \item \vspace{-0.1cm}
    \begin{enumerate}
    \item $\exists r \in R_s \cup R_d$ s.t. $f \in Head(r)$ and $\forall \varphi \in Body(r): +\delta \varphi \in P(1..i)$ and
    \item $-\Delta \lnot f \in P(1..i)$ and
    \item $\forall r' \in R$ s.t. $\lnot f \in Head(r')$, either:
      \begin{enumerate}
      \item $\exists \varphi \in Body(r')$ s.t. $-\delta \varphi \in P(1..i)$ or
      \item $\exists r^{\prime\prime} \in R$ s.t. $f \in Head(r^{\prime\prime})$ and $\forall \varphi \in Body(r^{\prime\prime}): +\delta f \in P(1..i)$ and $r^{\prime\prime} \succ r'$.
      \end{enumerate}
    \end{enumerate}
  \end{enumerate}

\item $-\delta$: if $P(i+1) = -\delta f$ then
  \begin{enumerate}
  \item $-\Delta f \in P(1..i)$ and
  \item \vspace{-0.1cm}
    \begin{enumerate}
    \item $\forall r \in R_s \cup R_d$ s.t. $f \in Head(r)$, $\exists \varphi \in Body(r): -\delta \varphi \in P(1..i)$ or
    \item $+\Delta \lnot f \in P(1..i)$ or
    \item $\exists r' \in R$ s.t. $\lnot f \in Head(r')$, and:
      \begin{enumerate}
      \item $\forall \varphi \in Body(r')$, $+\delta \varphi \in P(1..i)$ and
      \item $\forall r^{\prime\prime} \in R$ s.t. $f \in Head(r^{\prime\prime})$, either $\exists \varphi \in Body(r^{\prime\prime}): -\delta f \in P(1..i)$ or $r^{\prime\prime} \not\succ r'$
      \end{enumerate}
    \end{enumerate}
  \end{enumerate}
\end{itemize}

A literal $f$ is provable in $T$, denoted $T \models f$, if and only if there exists a proof $P$ in $T$ s.t. $f \in P$. For instance, intuitively, for $f$ to be strictly proven there needs to exist a strict rule $r \in R_s$ s.t. $f \in Head(r)$, and for any literal $l \in Body(r)$, there exists a strict rule $r' \in R_s$ s.t. $l \in Head(r')$, and so on recursively until a literal $t \in F$ is reached.

Ambiguity is the situation where a conflict between rules arises and the superiority relation does not indicate which rule should be used. 
\commentOCf{Je ne suis pas sûr de comprendre cette définition. Est-ce que une situation ambigue représente justement le cas où un litéral peut être affecté à plus d’un label selon les règles ci-dessus ?}
Different intuitions on how ambiguity should be solved have been introduced over the years; these intuitions modify subtly the way $+\delta$ and $-\delta$ are computed in order to account for how ambiguity, e.g., should be propagated to other literals. The interested reader might refer to \cite{Hecham2018} for both an intuitive and formal overview.

\subsection{From text to logic arguments}
Should we devote a section to discuss proposals to “translate” NL to logic?

From Besnard \& Hunter - Elements of Argumentation, Chapter 1. “Numerous textbooks have explained how individual arguments, as found in the real world, can be represented and analyzed by classical logic. The focus of these books is on what constitutes a valid logical argument. A paragon of such a textbook by Fisher [Fis88] includes many examples of arguments originally presented in philosophy and in politics using free text, together with a comprehensive explanation of how they can be translated into propositional logic. However, these textbooks tend to circumvent the more difficult issues of inconsistency, conflict, and counterarguments.
To address these more difficult issues, an excellent starting point for considering monological argumentation is Toulmin’s book [Tou58], with developments of Toulmin’s approach having been reviewed by van Eemeren et al. [vGK87]. Toulmin’s work is regarded as a precursor to much of the formal developments in argumentation systems found in the artificial intelligence field. For reviews of formalisms for argumentation systems in artificial intelligence, see [PV02, CML00].” And more about this in their Section 4.9.

\section{Reasons not to consider it known}
We saw that most formal approaches in argumentation consider that the attack relation is (are) known. 
We here point out reasons to lift this assumption.

More precisely, we want to challenge the assumption, left implicit in the literature, that some procedure exists that permits to extract from texts in natural language non-trivial conclusions, and that makes no assumption on the quality or content of the arguments in the text.

\subsection{A case study}
To transmit intuition, we give here a detailed example of a situation where 
two interpretations of given texts are natural, and yet lead to different 
conclusions. The example is taken from a timely social and scientific topic, namely the use of insect breeding as an ecological way to recycle and valorize bio-waste, known as ``entomoconversion'' \citep{FAO2021,Kialo2019}.

\subsubsection{Case description}
This year on 3 May 2021, the European Union approved a first insect, Tenebrio molitor, as novel food. This comes after several years of emulation and initiatives around entomoconversion in Western coutries, while in other parts of the world insect consumption has been practiced for centuries.

Entomoconversion is not only considered for its potential in food provision, but also as an ecological way to sanitize and recycle biowaste, such as urban waste, for various uses from feed and food to chemicals, to name a few. On the other hand, as an innovation it raises questions concerning the legal framework regulating this activity, the technical issues related to its implementation, the safety of the products delivered, the conditions of its economic viability, etc. Networks, projects, debates and literature provide documentation about these various aspects.

Part of these concerns are captured by the 2 models presented below.

The difference between models 1 and 2 is that model 2 makes explicit the economic consequence of increased costs. This modifies the argument ECO2 of model 1 into ECO2’ in model 2, which generates additional attacks.

\subsubsection{Model 1}
Arguments:
\begin{description}
	\item[COM1] ( reducedProductAttractivity $→$ reducedSells )  $→$  reducedEconomicBenefit
	\item[ECO1] ( biowasteUse $→$ reducedCosts )  $→$  increasedEconomicBenefit
	\item[ECO2] ( scalingUp $→$ (buildingCosts $\land$ labourCosts $\land$ automationCosts) )  $→$  increasedCosts
	\item[ECO3] ( highInsectGrowthRate $→$ highYield )  $→$  increasedEconomicBenefit
	\item[ECO4] ( highInsectFertility $→$ highYield )  $→$  increasedEconomicBenefit
\end{description}

Logical contradictions:
\begin{itemize}
	\item reducedCosts, increasedCosts $→ ⊥$
	\item reducedEconomicBenefit, increasedEconomicBenefit $→ ⊥$
\end{itemize}

Attacks: see \cref{fig:m1}.
\begin{figure}
	\caption{Attacks resulting from model 1}
	\label{fig:m1}
	\begin{tikzpicture}
		\tikzset{every node/.style={draw, ellipse}}
		\path node (COM1) {COM1};
		\path (COM1) ++(-3cm, -2cm) node (ECO1) {ECO1};
		\path (COM1) ++(0, -2cm) node (ECO3) {ECO3};
		\path (COM1) ++(3cm, -2cm) node (ECO4) {ECO4};
		\path (COM1) ++(0, -4cm) node (ECO2) {ECO2};
		\path[draw, line width=2pt, <->] (COM1) -- (ECO1);
		\path[draw, line width=2pt, <->] (COM1) -- (ECO3);
		\path[draw, line width=2pt, <->] (COM1) -- (ECO4);
		\path[draw, line width=2pt, ->] (ECO2) -- (ECO1);
	\end{tikzpicture}
\end{figure}

\subsubsection{Model 2}
Arguments include COM1, ECO1, ECO3, ECO4, but ECO2 changes to:
\begin{description}
	\item[ECO2’] [ ( scalingUp $→$ (buildingCosts $\land$ labourCosts $\land$ automationCosts) ) $→$ increasedCosts ] $→$ reducedEconomicBenefit
\end{description}

Logical contradictions are the same.

Attacks: see \cref{fig:m2}.
\begin{figure}
	\caption{Attacks resulting from model 2}
	\label{fig:m2}
	\begin{tikzpicture}
		\tikzset{every node/.style={draw, ellipse}}
		\path node (COM1) {COM1};
		\path (COM1) ++(-3cm, -2cm) node (ECO1) {ECO1};
		\path (COM1) ++(0, -2cm) node (ECO3) {ECO3};
		\path (COM1) ++(3cm, -2cm) node (ECO4) {ECO4};
		\path (COM1) ++(0, -4cm) node (ECO2) {ECO2’};
		\path[draw, line width=2pt, <->] (COM1) -- (ECO1);
		\path[draw, line width=2pt, <->] (COM1) -- (ECO3);
		\path[draw, line width=2pt, <->] (COM1) -- (ECO4);
		\path[draw, line width=2pt, <->] (ECO2) -- (ECO1);
		\path[draw, line width=2pt, <->] (ECO2) -- (ECO3);
		\path[draw, line width=2pt, <->] (ECO2) -- (ECO4);
	\end{tikzpicture}
\end{figure}

\subsubsection{Consequence of model differences}
Presented to an expert consortium \citep{Insect4City2020} using the preferred semantics of acceptability, Model 1 highlights argument ECO2 as a major argument of the debate, unattacked, skeptically accepted. It clearly weakens ECO1 which is attacked by ECO2 and undefended. It tends to orient the debate on the need for precise evaluation of scaling-up costs --which is the focus of argument ECO2.

On the other hand, Model 2 puts the question of costs back into the more general context of economic benefit. In this view, argument ECO2' (the new version of ECO2 in Model 2) is now faced to several arguments addressing economic benefit concerns, namely ECO1, ECO3 and ECO4, in a symmetric confrontation. This totally changes its status in the debate, which is now balanced between pessimistic arguments regarding entomoconversion's economic benefit (COM1, ECO2') and optimistic ones (ECO1, ECO3, ECO4), with a numeric advantage for the latter.

The differences between both model conclusions may be extremely puzzling for end-users who are not argumentation specialists. In this case, pedagogy and explanation of modeling choices and hypotheses are needed in order not to discredit the model.

\subsection{Example “short contraposition”}
\commentOCf{Exemple trouvé par Pierre (orca.cf.ac.uk/84295/1/short\_contraposition.pdf) à décrire.}

\subsection{Guaranteeing neutrality is incompatible with warrants}
In this section we wish to argue that modeling a debate in a way that is guaranteed to be neutral does not permit to obtain any warranted tree. 

We first observe, using a pair of examples, that neutrality requires to be robust considering some kinds of implicit claims. That is because some situations feature obvious disagreements which can be left implicit and we do not want the model to give the false impression that a claim is warranted just because some obvious implicit disagreement has not been included in the model.

Second, we argue, using further examples, that there is no neutral way to determine which implicit disagreements are obvious. Because of contrapositives, any argument attacking a claim can in principle be counter-attacked, and determining whether such a counter-attack should be included in the model requires to determine whether the person utterring the initial claim would consider the contrapositive as obvious. As far as we are aware, no automated procedure has been proposed to determine this. We provide an example which suggest that it is in general a very difficult matter indeed. Doing it on the basis of the informal intuition of the modeler is possible (and is done routinely), but this does not guarantee neutrality. Taking a sufficiently prudent view towards this difficult issue requires to include all possible implicit disagreements. This gives only models which have no warranted trees.

\begin{figure}
	\caption{The undercut tree arguing for $\mathit{s}$ in \cref{ex:ball}}
	\label{fig:ball}
	\begin{tikzpicture}
		\tikzset{every node/.style={draw, ellipse, execute at begin node=$,execute at end node=$}}
		\path node (Phi1) {\set{s}};
		\path (Phi1) ++(0, -2cm) node (Phi2) {\set{\mathit{b}, \mathit{b} → ¬\mathit{s}}};
		\path (Phi2) ++(0, -2cm) node (Phi3) {\mathit{p}, \mathit{p} → ¬\mathit{b}};
		\path (Phi3) ++(0, -2cm) node (Phi4) {¬\mathit{p}};
		\path[draw, <-] (Phi1) -- (Phi2);
		\path[draw, <-] (Phi2) -- (Phi3);
		\path[draw, <-] (Phi3) -- (Phi4);
	\end{tikzpicture}
\end{figure}
\begin{figure}
	\caption{The undercut tree arguing for $¬\mathit{s}$ in \cref{ex:ball} (incomplete)}
	\label{fig:ballnot}
	\begin{tikzpicture}
		\tikzset{every node/.style={draw, ellipse, execute at begin node=$,execute at end node=$}}
		\path node (Phi1) {\set{\mathit{b}, \mathit{b} → ¬\mathit{s}}};
		\path (Phi1) ++(0, -2cm) node (Phi2) {\set{\mathit{s}}};
		\path[draw, <-] (Phi1) -- (Phi2);
	\end{tikzpicture}
\end{figure}
\begin{example}[Ballistic expert]
	\label{ex:ball}
	A discussion happens about the possible cause of death of a person. Someone claims she has been shot by a sniper situated on the opposite building. A ballistic expert replies that no bullet could have reached the victim, considering her position, from the suspected position of the sniper. A counter-argument is raised saying that this in fact requires to use classical results in ballistics (as known in the scientific community), but they are in fact not known to be correct: a gigantic plot has led to them being falsely viewed as empirically validated, but they are not. The expert replies to this argument with a shrug by saying that there obviously exist no plot that render the use of ballistics illegitimate in this context.
	
	This disagreement can be modeled using the atoms 
	$\mathit{s}$ for \emph{A sniper shot the victim}, 
	$\mathit{b}$ for \emph{Ballistics can be used in this context} and 
	$\mathit{p}$ for \emph{A plot has led to a false impression of validity of ballistics}, 
	and using the model 
	$\Delta = \set{\mathit{s},\allowbreak \mathit{b}, \mathit{b} → ¬\mathit{s}, \mathit{p}, \mathit{p} → ¬\mathit{b}, ¬\mathit{p}}$.
	
	The unique undercut tree arguing for $\mathit{s}$ is displayed in \cref{fig:ball} and the unique undercut tree arguing for $¬\mathit{s}$ is displayed in \cref{fig:ballnot}.
\end{example}
This model seems to adequately capture the featured disagreement: neither the argumentation tree for $\mathit{s}$ nor the one for $¬\mathit{s}$ is warranted, reflecting the fact that who is right remains undetermined at this stage (if one wants to remain neutral, thus, not use the fact that one of the featured arguments seems weak).

Consider now the following variant of that scenario.
\begin{example}[Variation of \cref{ex:ball}]
	\label{ex:ball2}
	This example is identical to \cref{ex:ball}, except that the ballistic expert does not reply to the last argument (either because she has not had the opportunity to reply, or because she considers that the argument is not worth answering). A naïve model for this example is $\Delta^\text{l} = \set{\mathit{s},\allowbreak \mathit{b}, \mathit{b} → ¬\mathit{s}, \mathit{p}, \mathit{p} → ¬\mathit{b}}$.
\end{example}
This model seems to not adequately capture the opinion of the expert: the argumentation tree for the claim $\mathit{s}$ (identical to the one displayed in \cref{fig:ball} without the last node) is now warranted, as if the counter-argument of the expert, using $\mathit{b}$ and $\mathit{b} → ¬\mathit{s}$, had been convincingly rebutted by $\mathit{p}$ and $\mathit{p} → ¬\mathit{b}$. This model hence seems to not treat the two featured opinions on equal footing. Furthermore, this difference of conclusion compared to \cref{ex:ball} stems from the arbitrary fact that the expert did not utter her disagreement with $\Psi = \set{\mathit{p}, \mathit{p} → ¬\mathit{b}}$, although, arguably, it is clear that she believe $¬\Psi$ (thus $¬\mathit{p}$ or $¬(\mathit{p} → ¬\mathit{b})$), as $¬\Psi$ is entailed by her claim $\mathit{b}$. This difference of conclusion between both models may be considered to reflect an undesirable lack of robustness towards this kind of contingency.

As \citeauthor{besnard_elements_2000} observe, it is often necessary to add implicit statements to a text in natural language for it to make logical sense. 
Arguments are supposedly formalized making the enthymemes explicit (“An enthymeme is a form of reasoning in which some premises are implicit, most often because they are obvious” \citep[p.\ 41]{besnard_elements_2000}). They give the example “The baby no longer has her parents; therefore, she is an orphan”, which is formalized by making the further implicit claim explicit: “if a baby no longer has her parents, then she is an orphan”, which makes it a correct reasoning. 
More generally, people may omit obvious implications; may use “if” when obviously meaning “if and only if”, may use “or” with the obvious meaning “exclusive or”, and so on. Sticking to the text without attempting to interpret what the protagonists really mean would yield pedantic and uninteresting debates. It seems like the modeler should have the right to deviate from the raw text at least to consider what the debater “obviously mean”. In this example, the expert can reasonably be interpreted as obviously meaning that her use of classical knowledge in ballistics is legitimate, from the simple fact that she uses them. Whether or not she explicitly said it during the exchange of arguments should not be relevant.

These two arguments suggest to adopt the same model for \cref{ex:ball2} than for \cref{ex:ball}, and leave the debate formally unanswered, with both $\mathit{s}$ and $¬\mathit{s}$ unwarranted.

The problem is that this situation is general: when there is a disagreement about some conclusion, and some model seems to support some claim as warranted even though the model also permits to deduce its opposite claim, the question can in principle always be raised whether some implicit disagreement has been omitted, which would make the claim non warranted. The following example permit to understand intuitively why this problem is general.

The following example features the famous Tweety, where we wonder whether Tweety can fly.
\begin{figure}
	\caption{The undercut tree arguing for $¬\mathit{fT}$ in \cref{ex:fullTweety}}
	\label{fig:tweety}
	\begin{tikzpicture}
		\tikzset{every node/.style={draw, ellipse, execute at begin node=$,execute at end node=$}}
		\path node (Phi1) {\set{¬fT}};
		\path (Phi1) ++(0, -2cm) node (Phi2) {\set{\mathit{fb}, bT, \mathit{fb} \land \mathit{bT} → \mathit{fT}}};
		\path (Phi2) ++(0, -2cm) node (Phi3) {\mathit{p}, \mathit{p} → ¬\mathit{fb}};
		\path[draw, <-] (Phi1) -- (Phi2);
		\path[draw, <-] (Phi2) -- (Phi3);
	\end{tikzpicture}
\end{figure}
\begin{example}[Full Tweety]
	\label{ex:fullTweety}
	A bird expert talks with a man about a specific bird called Tweety on a given island that she visited. Although they agree that Tweety is a bird, she disagrees with the man when he claims that Tweety does not fly: because birds fly, she explains, Tweety flies. Interrogated afterwards, the man still thinks that the expert was wrong: he argues that penguins are birds, for example, but do not fly.
	
	Here is a naïve model of this disagreement.
	Our language uses the atoms 
	$\mathit{fb}$ for \emph{Birds fly}; 
	$\mathit{bT}$ for \emph{Tweety is a bird}; 
	$\mathit{fT}$ for \emph{Tweety flies}; 
	$\mathit{p}$ for \emph{penguins are birds but do not fly}.
	$\Delta = \set{¬\mathit{fT}, \mathit{fb}, \mathit{bT},\allowbreak \mathit{fb} \land \mathit{bT} → \mathit{fT}, \mathit{p},\allowbreak \mathit{p} → ¬\mathit{fb}}$.
\end{example}

The proposed model suggests that the man has given a decisive counter-argument to the expert, and that his claim is therefore shown to hold, at the current stage of the debate. However, this is arguable. Because a bird expert would not make such an obvious mistake as thinking that every bird fly, it is very reasonable to interpret her sentence as referring implicitly to the birds that can be observed in the specific island that is in the context of this discussion. There, it holds that every birds (that can be seen on the island) do fly. Under this interpretation, the expert would argue that penguins are not birds that can be seen on the island. We observe a structure identical to the one of \cref{ex:ball,ex:ball2}: because the expert claims some proposition $\phi$ (here, that birds in the relevant sense fly), and it is consensual that $\psi$ entails $¬\phi$ (here, that if penguins are birds in the relevant sense, then birds in that relevant sense do not fly), then taking the expert seriously (or merely being kind with the expert) requires to suppose that the expert thinks that $¬\psi$, and that possibly she did not say it because she finds it obvious. In other words, not including $¬\psi$ in the model in such a situation requires to take a non-neutral position against the expert by assuming that she does not hold that $¬\psi$, and that she thus did not think about the fact that her claim of $\phi$ entails $¬\psi$. (Not including $¬\psi$ in the model without making that hypothesis, for the simple reason that the expert did not explicitly say that she supports $¬\psi$, is a fall-back to the already discussed strategy of trying to stick to the text and not interpret what the debaters mean, which has the drawbacks we highlighted here above.)

Note that one may which to use different letters to represent the two meanings of the term birds: birds as understood in their general definition or birds with the implicit meaning: birds of this island. We stick to one letter for brievity, but this choice has no impact on our argument.

Our examples involve experts, but the requirements of neutrality apply equally when the debaters are not known as experts, for the same reasons: the modeler should not have to try to guess which entailments the debaters considered but left implicit when making their claims.

The following definitions and result indicate how our arguments generalize. (To be continued.)
\begin{definition}
	Let $I \subseteq \allformulas$ and $\Delta \subset \allformulas$ be sets of formulas, with $\Delta$ finite. The model $\Delta$ is said to be fragile considering the implicit claims in $I$ iff: $\exists \phi \in \Delta, I' \subseteq I, I' \text{ finite} \suchthat \phi$ is warranted in the model $\Delta$ but $\phi$ is not warranted in the model $\Delta' = \Delta \cup I'$.
	
	The model $\Delta$ is said to be robust considering the implicit claims in $I$ iff it is not fragile considering the implicit claims in $I$.
\end{definition}
\begin{definition}
	Given a finite set of formulas $\Psi = \set{\psi_1, \psi_2, …, \psi_n} \subset \allformulas$, say that a formula $p \in \allformulas$ has the form $¬\Psi$ when $p$ has the form $¬\psi_1 \lor ¬\psi_2 \lor … \lor ¬\psi_n$ or any permutation of these $n$ disjunctions.
	
	Consider some finite $\Delta \subset \allformulas$ and any $I \subseteq \allformulas$. Say that $I$ is a set of implicit disagreements from $\Delta$ iff $\forall \phi \in \Delta, \Psi = \set{\psi_1, \psi_2, …} \subseteq \Delta \suchthat (\Psi, ¬\phi) \text{ is an argument}: \exists p \in I \text{ that has the form } ¬\Psi$.
\end{definition}
Note that such a $p$ (a negation of the conjunction of the propositions in $\Psi$) can arguably be considered as a natural answer to the argument $(\Psi, ¬\phi)$ under the provided conditions, as $p$ is then entailed by $\phi$, which is in $\Delta$.

A trivial warrant is defined as any claim that is not contested in $\Delta$.
\begin{definition}
	A model $\Delta$ has \emph{only trivial warrants} iff its set of warranted propositions is $\set{\phi \in \allformulas \suchthat \exists \Phi \subseteq \Delta \suchthat \Phi \vdash \phi \land \nexists \Psi \subseteq \Delta \suchthat \Psi \nvdash \bot \text{ and } \Psi \vdash ¬\phi}$.
\end{definition}
Note that $\phi$ is a trivial warrant iff some undercut tree arguing for $\phi$ contain a single node.

\begin{conjecture}
	Consider a finite $\Delta \subseteq \allformulas$ and any set of implicit disagreements $I$. If $\Delta$ is robust considering the implicit claims in $I$, then $\Delta$ has only trivial warrants.
\end{conjecture}
\begin{proof}
	Consider $\Delta$ not having only trivial warrant. Consider some $\phi \in \Delta$ that is not a trivial warrant. 
	Let $\mathcal{U}$ denote the set of undercut trees arguing for $\phi$ with an undefeated root. 
	Given an undercut tree $u$, let $u^{-1}(\emptyset)$ denote its set of leaves (the nodes with no children).
	Given a node $\Psi$ in an undercut tree $u$, let $n(\Psi, u)$ denote a formula of the form $¬\Psi$ if some formula of that form is not used in $u$, a formula of the form $¬¬(¬\Psi)$ if every formulas of the form $¬\Psi$ are used in $u$ but some formula of the form $¬¬(¬\Psi)$ isn’t, and so on. (Note that as $u$ is finite, there is necessarily some formula of the form $(¬¬)^k(¬\Psi)$ that is not used in $u$).
	Define $\Delta' = \Delta \cup \set{n(\Psi, u) \suchthat u \in \mathcal{U} \text{ and } \Psi \in u^{-1}(\emptyset)}$.
	 
	We show that every undercut tree arguing for $\phi$ in $\Delta'$ has a defeated root.
	Note that every $u \in \mathcal{U}$ have at least two nodes, as $\phi$ is not a trivial warrant.
	
	In fact, the stated conjecture is false, because of some trees that use rebuttals. Consider $a$ undercut by $¬¬¬a$ undercut by $¬¬a$. Here, adding $¬¬¬a$ does not change anything. Similarly, with root $a$ and undercut $\set{¬b, ¬b → ¬a}$ undercut by $b$, there seems to be nothing we can add.
	
	We’d rather add the implication $\phi → ¬\Psi$.
\end{proof}
	
For the reasons stated in this section, we consider being guaranteed neutral as requiring to not use models that are fragile considering implicit disagreements. It follows that neutrality is incompatible with obtaining non-trivial warranted propositions.

\begin{remark}
	Being neutral is less stringent than being guaranteed neutral. We use the latter phrase to refer to the requirement of using a modeling procedure that is automatically verifiable by third-parties. A modeler can be neutral and not guaranteed neutral: she may use her intuition and unformalized knowledge about the context of a debate to determine which claims are implicit and which ones are arguably not. The usual example about Tweety features such situation: in many circumstances, the modeler would be justified in considering that the counter-argument about penguins indeed attacks the argument about birds supposedly implying an ability to fly, because there may be a presumption, unless the context indicates it, that the “right” meaning of bird in that context is the one of general birds, and the modeler may use the fact that she knows that the argument about penguins is correct in that sense. 
	
	Using intuition and unformalized knowledge is routinely practiced in obtaining formal models. 
	We do not claim that this is necessarily a mistake. We only claim that this requires to use “common sense”, which is notoriously difficult to define precisely, and does therefore not guarantee neutrality, in the sense that someone seeing the result of the model may still doubt whether the modeler considered implicit claims correctly; and if a suspicion to the contrary is raised, no criteria are known to determine who is right. This is especially a problem in situations where trusting the modeler is undesirable.
	
	To summarize: if one may use $\Psi$ to attack $\phi$, when $\Psi \vdash ¬\phi$, with $\phi \in \Delta$, then neutrality requires to also authorize to use $¬\Psi$, which is obtained by the contrapositive fact that $\phi \vdash ¬\Psi$.
	Proceeding otherwise requires to assume that the entailment from $\Psi$ to $¬\phi$ is justified whereas the contrapositive one is not, which may not be justified by logic alone, although it may be justified on grounds related to which facts hold in the world or what one may assume the protagonists in a debate know and ignore. In the famous classical Tweety example (and assuming consensual that penguins do not fly), it may be justified to use the fact that pengins being birds, birds do not fly; and not justified to use the contrapositive fact that “birds fly” logically entail that penguins are not birds. But this does not result from logic, but from our knowledge that in fact, that birds fly is an incorrect claim which should be defeated by penguins being birds, and not the other way around. Our conclusion, thus, is that non-trivial warrants can be obtained only at the price of the modeler deciding who is right, not as a result of a logical model using only logic deduction.
\end{remark}

There is a need for an approach that makes the correctness of the attack relations a scientific (meaning, here, a precise and falsifiable) claim.

\subsection{More discussion}
\commentOCf{Contenu de cette section à reformuler ou effacer à terme…}

The assumption we want to discuss here is the following.
Many articles start with a postulated known attack relation or logic representation of arguments. It is natural to wonder where this information might come from, and we suppose, although this is seldom discussed explicitly, that the assumption is that it is more or less easy to obtain such relations from the natural language form in which arguments generally arise in the wild. 
The postulate that we want to argue against here is that by inspecting the arguments and using common sense, either the attack relation can be seen immediately, or a logical representation of the arguments can unambigiously be obtained, itself yielding a non-trivial attack structure by logical manipulation.
By non-trivial, we mean here that the structures that are obtained do permit, at least sometimes, to discriminate between a claim and its negation, even though both the claim and its negation can be deduced from arguments in the text. (Intuitively, we want to exclude cases of obtaining entirely symmetric conclusions.)

Accordingly, we focus on the case of arguments given under natural language. 

We make the following hypothesis of robustness against interpretion. The set of possible interpretations is $I$. If an interpretation leads to $\Delta$, and some argument in the DB says $\Phi$ and is attacked by $\Psi$, and it is possible to deduce, from the DB, using elements from $\Phi$ and $\Psi$, a counter-argument, then it is included in the set of interpretations.

Undoubtedly, some situations let one determine easily the attack relation, by simple appeal to an obvious consensus. In the classical textbook example of Alice saying that “it will rain tomorrow because the weather forecast on the BBC said so” and Bob replying that “it will not rain tomorrow because the weather forecast on Channel 4 said so”, the situation is clear, and a simple appeal to consensus permits to solve any doubt to the contrary: give these arguments to any two persons, and they will agree that these arguments attack each other. Such trivial situations, although being possibly adequate for illustrative purposes of technical definitions, are however hardly representative of the complexity of the debates that an argumentation framework must be able to tackle. It seems reasonable to demand of argumentation theory to help us think about debates involving subtle, complicated arguments. For example, a framework could claim to be able to help a medical doctor take a decision about which treatment to prescribe, or help an individual to choose which political party she will vote for.

Our main claim is that there is no neutral formal way to determine an attack relation that is not symmetric. In other words, obtaining a non-symmetric attack relation requires the analyst to adopt a position in the debate, in the sense of considering some claims as “winning” over others in a way that cannot be justified in purely formal terms.

First, note that one can always interpret “kindly” any argument, by filling in (considering as implicit) any supplementary claim that is required to make it work. To illustrate, assume John says: “because the apple detached from the tree, it fell on the ground”. The antecedent is not sufficient to bring the consequent: in case of storm, for example, the apple could detach from the tree and (temporarily) not fall on the ground. Depending on the circumstances of the speech, however, it might be perfectly reasonable to assume that John really meant: “because the apple detached from the tree and there was no storm on that day, it fell on the ground”. Indeed, if a required element in a reasoning is missing, it may be because it is an implicit assumption (that perhaps John knows is shared by the audience, hence does not consider useful to say). Now, interpreting kindly any argument will constantly lead to abstain from any conclusion: any argument that attacks any other will be attacked in return. Indeed, imagine someone says: “as there was storm on that day, the apple did in fact not fell on the ground”. Interpreting John’s text kindly, we would conclude that these arguments attack each other, and will not be able to conclude anything.

If we want to avoid this unfortunate consequence of extreme permissivism, we must restrict which attacks we (as modelers) we consider as “plausible”. In the famous Tweety example, we consider likely that the defeasible rule Bird(x) => Fly(x) is abusively used for building the argument that Bird(Tweety) and therefore Fly(Tweety), in a case where Tweety is a penguin; and we obtain a not too permissive model where the argument Penguin(Tweety) attacks the first one. But we could, again, conclude otherwise: if, unknown to the modeler, the person uttering the sentence in favor of Tweety flying is talking about an island where there are only birds that fly, then the use of the rule is authorized, and the person saying that Tweety is a Penguin must be mistaken. It is a choice of the modeler to conclude otherwise.

This shows that one will either be overly permissive, or need a trusted source of knowledge (that defines a hierarchy of claims by plausibility, say).

Another way to view this tension is to observe that we need to interpret what John says: we do not know what is missing from his reasoning because he did not realize some supplementary hypothesis was required (and therefore possibly obtained a mistaken conclusion), or because he thought it was sufficiently clear for the audience that the hypothesis was holding in the context of his speech.

More generally, using the scheme requires to add a priori knowledge, which is usually left implicit in the literature but is nonetheless required. Defeasible rule: A implies B. Explicit rule: A and X imply B. Attack: not X. But we might instead make the first speaker win the argument by instead being more kind to him and considering that the mere fact that he used the rule A implies B means that he implicitly thought it clear that X hold as well. (This kind hypothesis may seem reasonable in some contexts, such as when the speaker is an expert in the relevant domain.) Also, it could be that there is another way to complete the defeasible rule A implies B, by saying A and Y imply B. In that case, not(X) is uneffective against the first speaker. When the modeler considers that an argument attacks another one, and not conversely, she is inevitably taking a (possibly implicit) position in favor of some of the arguments.

We can rephrase this by saying that a first and important reason to not consider the attack relation known a priori is that considering it known a priori seems to require to assume that the attack relation is objective, thus in particular, that whether an argument attacks another one does not depend on who is listening, or on a choice of interpretation. The objectivity assumption permits to declare that an argument “truly” attacks another one, even though some given person may fail to realize it or may even believe the contrary. 

Real-life debates feature omissions and implicit statements. It may be fundamentally undetermined (and not simply unknown) which statements exactly have been omitted so as to connect an argument to what the debater has said previously. The debater himself may be hard pressed to explain it, if asked: we human being may not always form fully precise statements mentally before uttering them, or if we do, we do not necessarily have access to the precise version of the statements. A mathematician might “feel” that a proposition is true long before knowing how to prove it. A person may intuitively “see” how one argument supports or attacks another one without being able to fill in all the gaps and make it a precise reasoning. Or there may be multiple reasonings that allow to fill in the gaps, and the debater may erroneously think that they are just different ways of expressing the same idea; whereas it could be that one phrasing features a mistake in reasoning whereas another one does not. It is the norm with philosophers debating of complex ideas to consider multiple versions of their opponent’s claim, trying to interpret them “kindly”, that is, filling what they see as gaps in their opponent’s reasoning in ways that make their opponent’s view point as solid as possible (cite Denett). It seems hard to justify that one of these versions in particular would objectively be the right version. And if there are indeed multiple interpretations of a statement that correspond to multiple precise arguments, it seems unlikely that they all would lead to the same attack relation when considered within a set of related arguments. Therefore, in some interesting cases, the attack relation is not objectively determined.

Another important reason to not consider the attack relation known a priori relates to the understanding of the attack relation itself. 
Even when arguably “what the debater means objectively” is well defined, one may, in some applications, be interested in what some given listener understands from the debate, rather than by what the debater means. 
This is an important distinction in our context because the attack relation representing what the listener understands may differ from the one representing what the debater means.
The listener may not know the meaning of some words used by the debaters, or she may understand them differently than what the debaters have in mind. 
The listener may not have the deductive power to understand what could otherwise be considered a trivial implication of the argument. 
The listener may lack some required knowledge.

Translating natural language arguments into a logic language does not constitute a solution to these problems, as the problems exist, unchanged, at the level of translation. When there are multiple ways of interpreting some implicit information present in an argument (as intended by the debater, or as understood by the listener), the modeler must choose one when translating it in a formal, precise language, with the risk of introducing an arbitrary choice or losing neutrality.

A practical argument is that encoding the arguments logically require important expertise, which may be unavailable or cost too much.

\commentOC{To discuss: which points require examples? How complex (toy examples VS realistic examples)? Consider an example of a logic encoding with different conclusions?}

Practical studies showed that rejected arguments are not necessarily to be interpreted as non-receivable, but rather as rough ideas that need to be refined \cite{EJDP2018, Bourguet2013} and can serve as initial pillars of great importance for the construction of the argumentation system. Talk about obtaining judgements of individuals, …

\section{what if not known a priori?}
We just argued that it may be relevant or convenient to not assume we know the attack relation. In this section we provide some ideas of subject of studies that do not require this assumption, that pertains to computer science, and does not require advanced natural language processing capabilities.

(To be continued.)
We consider that what counts is what the receiver of the argument says and we propose to study several aspects related to this perspective: how to find decisive arguments in this sense; is this consensual; what are appropriate methods, statistical or others; what are the protocols that must be used to gather arguments; how can we test that every arguments that are possibly relevant have been considered?

\section{a sketch of a possible approach}
Let people come to a website and argue and indicate which arguments theirs answer, then confront this to the opinion of other visitors
\commentRT{Raccrocher Nicolas ici ?}

\section{Others}
We should decide whether to include some of these points in the text or adding sections about them or dropping them.
\begin{itemize}
	\item possibly, contrast with persuasion?
	\item link to explainable AI (should focus on decisive arguments), exploitation of weaknesses of will or sub-conscious behavioral patterns (youtube autoplay; nudge; anchoring) VS reflective arguments (advocacy), and the like? Link to deliberative democracy?
	\item review existing experimental work in FAT? Other related works? Philosophical approaches (Rawls, Habermas)?
\end{itemize}

\bibliography{empirical}

\end{document}
